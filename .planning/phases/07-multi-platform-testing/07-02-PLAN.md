---
phase: 07-multi-platform-testing
plan: 02
type: checkpoint:human-verify
wave: 2
depends_on: [07-01]
files_modified:
  - scripts/test-platform-install.js
  - scripts/test-platform-commands.js
  - scripts/test-regression.js
  - scripts/test-tool-mapping.js
  - scripts/test-platform-detection.js
  - test-environments/test-results.json
autonomous: false
must_haves:
  - "Installation tested on all 3 platforms (Copilot, Claude, Codex)"
  - "All 29 commands install successfully on each platform (87 total installations)"
  - "Commands are discoverable using platform-specific invocation"
  - "Tool mapping verified: Claude tools map correctly to platform equivalents"
  - "Platform-specific content renders correctly in generated files"
  - "Platform detection with graceful fallbacks tested"
  - "npm run install --all parallel installation tested"
  - "Legacy fallback behavior verified when specs missing"
  - "Regression testing expanded to 15-20 commands covering complexity spectrum"
  - "Test results captured in structured JSON format"
---

# Phase 7, Plan 2: Platform Testing + Execution

**Objective:** Create platform testing scripts and execute comprehensive multi-platform verification with human checkpoints for command testing

## Context

This plan implements the actual testing workflow across 3 platforms. Sequential execution (Copilot ‚Üí Claude ‚Üí Codex) with automated installation testing and manual command verification.

**From Context:** Hybrid approach ‚Äî automated install, manual command testing. 100% success threshold (all 29 commands on all 3 platforms). Failure handling: continue testing, batch all failures for review.

**Test strategy:**
1. Automated: Installation success/failure detection
2. Manual: Command execution and output verification (requires human judgment)
3. Automated: Regression comparison (legacy vs spec output diff)

**Why manual verification needed:** Command behavior requires contextual judgment (e.g., "Does gsd-progress show correct routing?", "Does gsd-new-project spawn 5 agents?"). Not easily automatable without extensive mocking.

## Tasks

<task name="create-platform-testing-scripts" type="auto">
  <files>
    scripts/test-platform-install.js
    scripts/test-platform-commands.js
    scripts/test-regression.js
  </files>
  
  <action>
Create 3 testing scripts for platform installation, command execution, and regression testing.

**1. Create scripts/test-platform-install.js:**

```javascript
#!/usr/bin/env node
const { execSync } = require('child_process');
const fs = require('fs');
const path = require('path');

const platforms = [
  { name: 'copilot', cmd: 'npm run install:copilot' },
  { name: 'claude', cmd: 'npm run install:claude' },
  { name: 'codex', cmd: 'npm run install:codex' }
];

async function testPlatformInstall(platform) {
  console.log(`\n${'='.repeat(60)}`);
  console.log(`Testing ${platform.name} installation`);
  console.log('='.repeat(60));
  
  const testDir = path.join(__dirname, '..', 'test-environments', `${platform.name}-test`, 'get-shit-done');
  
  if (!fs.existsSync(testDir)) {
    return {
      platform: platform.name,
      success: false,
      error: 'Test environment not found. Run: node test-environments/setup-test-env.js'
    };
  }
  
  try {
    console.log(`üì¶ Installing in ${platform.name}-test/get-shit-done...`);
    
    const output = execSync(platform.cmd, {
      cwd: testDir,
      encoding: 'utf8',
      stdio: 'pipe'
    });
    
    console.log(output);
    
    // Verify installation
    const result = verifyInstallation(platform.name, testDir);
    
    if (result.success) {
      console.log(`‚úÖ ${platform.name}: ${result.skillsGenerated} commands installed`);
    } else {
      console.log(`‚ùå ${platform.name}: Installation failed`);
      console.log(`   Errors: ${result.errors.join(', ')}`);
    }
    
    return {
      platform: platform.name,
      success: result.success,
      skillsGenerated: result.skillsGenerated,
      errors: result.errors
    };
    
  } catch (error) {
    console.log(`‚ùå ${platform.name}: Installation threw error`);
    console.log(`   ${error.message}`);
    
    return {
      platform: platform.name,
      success: false,
      error: error.message
    };
  }
}

function verifyInstallation(platform, testDir) {
  const platformPaths = {
    copilot: '.github/copilot/skills',
    claude: '.claude/get-shit-done',
    codex: '.codex/skills'
  };
  
  const skillsDir = path.join(testDir, platformPaths[platform]);
  
  if (!fs.existsSync(skillsDir)) {
    return {
      success: false,
      skillsGenerated: 0,
      errors: [`Skills directory not found: ${platformPaths[platform]}`]
    };
  }
  
  const skills = fs.readdirSync(skillsDir).filter(f => f.startsWith('gsd-') && f.endsWith('.md'));
  const expectedCount = 29; // Total GSD commands
  
  const success = skills.length === expectedCount;
  const errors = [];
  
  if (skills.length < expectedCount) {
    errors.push(`Expected ${expectedCount} commands, found ${skills.length}`);
  }
  
  return {
    success,
    skillsGenerated: skills.length,
    errors
  };
}

async function main() {
  console.log('üöÄ Testing platform installations\n');
  console.log('Testing order: Copilot ‚Üí Claude ‚Üí Codex\n');
  
  const results = [];
  
  for (const platform of platforms) {
    const result = await testPlatformInstall(platform);
    results.push(result);
    
    // Pause between platforms
    if (platform !== platforms[platforms.length - 1]) {
      console.log('\n‚è∏Ô∏è  Pausing 2 seconds before next platform...\n');
      await sleep(2000);
    }
  }
  
  // Summary
  console.log('\n' + '='.repeat(60));
  console.log('Installation Test Summary');
  console.log('='.repeat(60));
  
  for (const result of results) {
    const status = result.success ? '‚úÖ' : '‚ùå';
    const skills = result.skillsGenerated || 0;
    console.log(`${status} ${result.platform}: ${skills}/29 commands`);
    
    if (!result.success && result.error) {
      console.log(`   Error: ${result.error}`);
    }
    if (!result.success && result.errors) {
      result.errors.forEach(err => console.log(`   - ${err}`));
    }
  }
  
  // Write results
  const resultsPath = path.join(__dirname, '..', 'test-environments', 'install-results.json');
  fs.writeFileSync(resultsPath, JSON.stringify(results, null, 2));
  console.log(`\nüìÑ Results written to: test-environments/install-results.json`);
  
  // Exit code
  const allSuccess = results.every(r => r.success);
  if (!allSuccess) {
    console.log('\n‚ùå Some installations failed');
    process.exit(1);
  }
  
  console.log('\n‚úÖ All installations successful!');
}

function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

if (require.main === module) {
  main().catch(err => {
    console.error('‚ùå Test script error:', err);
    process.exit(1);
  });
}

module.exports = { testPlatformInstall, verifyInstallation };
```

**2. Create scripts/test-platform-commands.js:**

```javascript
#!/usr/bin/env node
const fs = require('fs');
const path = require('path');

// List of 29 GSD commands to test
const commands = [
  // High complexity (3)
  'gsd-new-project',
  'gsd-execute-phase',
  'gsd-new-milestone',
  
  // Medium complexity (4)
  'gsd-plan-phase',
  'gsd-research-phase',
  'gsd-debug',
  'gsd-map-codebase',
  
  // Simple commands (22)
  'gsd-help',
  'gsd-progress',
  'gsd-verify-work',
  'gsd-discuss-phase',
  'gsd-pause-work',
  'gsd-resume-work',
  'gsd-add-phase',
  'gsd-insert-phase',
  'gsd-remove-phase',
  'gsd-add-todo',
  'gsd-check-todos',
  'gsd-complete-milestone',
  'gsd-audit-milestone',
  'gsd-plan-milestone-gaps',
  'gsd-archive-milestone',
  'gsd-restore-milestone',
  'gsd-list-milestones',
  'gsd-list-phase-assumptions',
  'gsd-verify-installation',
  'gsd-whats-new',
  'gsd-update'
];

async function testPlatformCommands(platform) {
  console.log(`\n${'='.repeat(60)}`);
  console.log(`Manual Command Testing: ${platform}`);
  console.log('='.repeat(60));
  console.log();
  console.log(`Test environment: test-environments/${platform}-test/get-shit-done`);
  console.log();
  console.log('Instructions:');
  console.log('1. Open terminal in test environment');
  console.log('2. Test each command below');
  console.log('3. Verify command is discovered and executes');
  console.log('4. Note any failures or unexpected behavior');
  console.log();
  console.log('Platform-specific invocation:');
  
  if (platform === 'copilot') {
    console.log('  GitHub Copilot: Type command name, press Tab for autocomplete');
  } else if (platform === 'claude') {
    console.log('  Claude Code: Type /gsd- and use autocomplete');
  } else if (platform === 'codex') {
    console.log('  Codex CLI: Type $gsd- and use autocomplete');
  }
  
  console.log();
  console.log('Commands to test:');
  console.log('-'.repeat(60));
  
  commands.forEach((cmd, idx) => {
    console.log(`${idx + 1}. ${cmd}`);
  });
  
  console.log();
  console.log('Expected behavior:');
  console.log('- Command appears in autocomplete');
  console.log('- Help text displays correctly');
  console.log('- Command executes (even if requires args/context)');
  console.log();
  console.log('‚ö†Ô∏è  This is a MANUAL testing checkpoint');
  console.log('üìù Document failures in test-environments/test-results.json');
  console.log();
}

async function main() {
  console.log('üß™ Platform Command Testing Guide\n');
  console.log('This script provides instructions for MANUAL command testing.\n');
  console.log('Why manual? Commands require context and human judgment to verify behavior.\n');
  
  const platforms = ['copilot', 'claude', 'codex'];
  
  for (const platform of platforms) {
    await testPlatformCommands(platform);
    
    if (platform !== platforms[platforms.length - 1]) {
      console.log('\nPress Enter to continue to next platform...');
      // In actual use, would wait for input
      // For now, just show instructions
    }
  }
  
  console.log('='.repeat(60));
  console.log('Testing Complete');
  console.log('='.repeat(60));
  console.log();
  console.log('Next steps:');
  console.log('1. Update test-environments/test-results.json with findings');
  console.log('2. Run regression tests: node scripts/test-regression.js');
  console.log('3. Proceed to Plan 3 for analysis and reporting');
}

if (require.main === module) {
  main().catch(err => {
    console.error('‚ùå Script error:', err);
    process.exit(1);
  });
}

module.exports = { commands };
```

**3. Create scripts/test-regression.js:**

```javascript
#!/usr/bin/env node
const { execSync } = require('child_process');
const fs = require('fs');
const path = require('path');
const stripAnsi = require('strip-ansi');
const diff = require('diff');

// Commands safe to run without affecting project state
const safeCommands = [
  'gsd-help',
  'gsd-verify-installation',
  'gsd-list-milestones',
  'gsd-whats-new',
  'gsd-list-phase-assumptions',
  'gsd-check-todos',
  // Added for expanded TEST-08 coverage (15 commands total)
  'gsd-progress',
  'gsd-pause-work',
  'gsd-resume-work',
  'gsd-list-phase-assumptions',
  'gsd-archive-milestone',
  'gsd-restore-milestone',
  'gsd-audit-milestone',
  'gsd-complete-milestone',
  'gsd-update'
];

async function testRegression(platform) {
  console.log(`\n${'='.repeat(60)}`);
  console.log(`Regression Testing: ${platform}`);
  console.log('='.repeat(60));
  
  const testDir = path.join(__dirname, '..', 'test-environments', `${platform}-test`, 'get-shit-done');
  const results = [];
  
  for (const command of safeCommands) {
    console.log(`\nüìã Testing: ${command}`);
    
    try {
      // Note: This is a simplified regression test
      // Full implementation would need platform-specific invocation
      // and careful output comparison
      
      // For now, verify command exists in generated output
      const platformPaths = {
        copilot: '.github/copilot/skills',
        claude: '.claude/get-shit-done',
        codex: '.codex/skills'
      };
      
      const skillFile = path.join(testDir, platformPaths[platform], `${command}.md`);
      
      if (fs.existsSync(skillFile)) {
        console.log(`  ‚úÖ Generated file exists`);
        
        const content = fs.readFileSync(skillFile, 'utf8');
        const hasName = content.includes(`name: ${command}`);
        const hasDescription = content.includes('description:');
        
        if (hasName && hasDescription) {
          console.log(`  ‚úÖ File structure valid`);
          results.push({
            command,
            platform,
            success: true
          });
        } else {
          console.log(`  ‚ùå File structure invalid`);
          results.push({
            command,
            platform,
            success: false,
            error: 'Missing required fields'
          });
        }
      } else {
        console.log(`  ‚ùå Generated file not found`);
        results.push({
          command,
          platform,
          success: false,
          error: 'File not found'
        });
      }
      
    } catch (error) {
      console.log(`  ‚ùå Error: ${error.message}`);
      results.push({
        command,
        platform,
        success: false,
        error: error.message
      });
    }
  }
  
  return results;
}

async function main() {
  console.log('üîç Regression Testing Suite\n');
  console.log(`Testing ${safeCommands.length} safe commands across 3 platforms\n`);
  
  const platforms = ['copilot', 'claude', 'codex'];
  const allResults = [];
  
  for (const platform of platforms) {
    const results = await testRegression(platform);
    allResults.push(...results);
  }
  
  // Summary
  console.log('\n' + '='.repeat(60));
  console.log('Regression Test Summary');
  console.log('='.repeat(60));
  
  const successCount = allResults.filter(r => r.success).length;
  const totalTests = allResults.length;
  
  console.log(`\n‚úÖ Passed: ${successCount}/${totalTests}`);
  console.log(`‚ùå Failed: ${totalTests - successCount}/${totalTests}`);
  
  if (successCount < totalTests) {
    console.log('\nFailed tests:');
    allResults.filter(r => !r.success).forEach(r => {
      console.log(`  - ${r.platform}/${r.command}: ${r.error}`);
    });
  }
  
  // Write results
  const resultsPath = path.join(__dirname, '..', 'test-environments', 'regression-results.json');
  fs.writeFileSync(resultsPath, JSON.stringify(allResults, null, 2));
  console.log(`\nüìÑ Results written to: test-environments/regression-results.json`);
  
  if (successCount < totalTests) {
    process.exit(1);
  }
  
  console.log('\n‚úÖ All regression tests passed!');
}

if (require.main === module) {
  main().catch(err => {
    console.error('‚ùå Regression test error:', err);
    process.exit(1);
  });
}

module.exports = { testRegression, safeCommands };
```

**4. Make scripts executable:**

```bash
chmod +x scripts/test-platform-install.js
chmod +x scripts/test-platform-commands.js
chmod +x scripts/test-regression.js
```
  </action>
  
  <verify>
```bash
# Verify scripts exist
ls -la scripts/test-platform-install.js
ls -la scripts/test-platform-commands.js
ls -la scripts/test-regression.js

# Test installation script (automated)
node scripts/test-platform-install.js

# Should output:
# - Installation progress for each platform
# - Verification of 29 commands installed
# - install-results.json created
```
  </verify>
  
  <done>
- [x] scripts/test-platform-install.js automates installation testing across 3 platforms
- [x] scripts/test-platform-commands.js provides manual testing guide for 29 commands
- [x] scripts/test-regression.js validates generated files match expected structure
- [x] Scripts are executable and can run independently
  </done>
</task>

<task name="test-tool-mapping-and-content-rendering" type="auto">
  <files>
    scripts/test-tool-mapping.js
  </files>
  
  <action>
Create script to test tool mapping verification (PLAT-05) and platform-specific content rendering (PLAT-06).

**Create scripts/test-tool-mapping.js:**

```javascript
#!/usr/bin/env node
const fs = require('fs');
const path = require('path');

// Tool mapping: Claude ‚Üí Platform equivalents
const toolMappings = {
  copilot: {
    view: 'read_file',
    edit: 'edit_file',
    create: 'create_file',
    bash: 'execute_command',
    task: 'delegate_task'
  },
  claude: {
    view: 'view',
    edit: 'edit',
    create: 'create',
    bash: 'bash',
    task: 'task'
  },
  codex: {
    view: 'file.read',
    edit: 'file.modify',
    create: 'file.create',
    bash: 'shell.execute',
    task: 'agent.spawn'
  }
};

async function testToolMapping(platform) {
  console.log(`\n${'='.repeat(60)}`);
  console.log(`Testing Tool Mapping: ${platform}`);
  console.log('='.repeat(60));
  
  const testDir = path.join(__dirname, '..', 'test-environments', `${platform}-test`, 'get-shit-done');
  const platformPaths = {
    copilot: '.github/copilot/skills',
    claude: '.claude/get-shit-done',
    codex: '.codex/skills'
  };
  
  const skillsDir = path.join(testDir, platformPaths[platform]);
  
  if (!fs.existsSync(skillsDir)) {
    console.log(`‚ùå Skills directory not found: ${skillsDir}`);
    return { platform, success: false, error: 'Skills directory not found' };
  }
  
  const skills = fs.readdirSync(skillsDir).filter(f => f.startsWith('gsd-') && f.endsWith('.md'));
  const results = { platform, tested: 0, passed: 0, failed: 0, failures: [] };
  
  for (const skillFile of skills.slice(0, 5)) { // Sample 5 skills
    const content = fs.readFileSync(path.join(skillsDir, skillFile), 'utf8');
    results.tested++;
    
    // Check tool declarations
    const toolsMatch = content.match(/tools:\s*\[(.*?)\]/s);
    if (!toolsMatch) {
      results.failed++;
      results.failures.push({
        skill: skillFile,
        issue: 'No tools array found'
      });
      continue;
    }
    
    const declaredTools = toolsMatch[1].split(',').map(t => t.trim().replace(/['"]/g, ''));
    
    // Verify each tool is properly mapped
    let allToolsValid = true;
    for (const tool of declaredTools) {
      const expectedMapping = toolMappings[platform][tool];
      if (!expectedMapping) {
        allToolsValid = false;
        results.failures.push({
          skill: skillFile,
          tool,
          issue: `No mapping defined for platform ${platform}`
        });
      } else if (platform !== 'claude') {
        // For non-Claude platforms, verify tool is mapped in content
        // (This is simplified - actual implementation would check tool descriptions)
        const hasMapping = content.includes(expectedMapping) || content.includes(tool);
        if (!hasMapping) {
          console.log(`  ‚ö†Ô∏è  ${skillFile}: Tool '${tool}' not clearly mapped to '${expectedMapping}'`);
        }
      }
    }
    
    if (allToolsValid) {
      results.passed++;
    } else {
      results.failed++;
    }
  }
  
  console.log(`\n‚úÖ Passed: ${results.passed}/${results.tested}`);
  if (results.failed > 0) {
    console.log(`‚ùå Failed: ${results.failed}`);
    results.failures.forEach(f => {
      console.log(`  - ${f.skill}: ${f.issue || f.tool + ' - ' + f.issue}`);
    });
  }
  
  return results;
}

async function testContentRendering(platform) {
  console.log(`\n${'='.repeat(60)}`);
  console.log(`Testing Platform-Specific Content: ${platform}`);
  console.log('='.repeat(60));
  
  const testDir = path.join(__dirname, '..', 'test-environments', `${platform}-test`, 'get-shit-done');
  const platformPaths = {
    copilot: '.github/copilot/skills',
    claude: '.claude/get-shit-done',
    codex: '.codex/skills'
  };
  
  const skillsDir = path.join(testDir, platformPaths[platform]);
  const skills = fs.readdirSync(skillsDir).filter(f => f.startsWith('gsd-') && f.endsWith('.md'));
  
  const results = { platform, tested: 0, passed: 0, failed: 0, issues: [] };
  
  // Sample check: Verify platform-specific conditionals rendered correctly
  for (const skillFile of skills.slice(0, 5)) {
    const content = fs.readFileSync(path.join(skillsDir, skillFile), 'utf8');
    results.tested++;
    
    let passed = true;
    
    // Should NOT contain other platform conditionals
    const otherPlatforms = ['claude', 'copilot', 'codex'].filter(p => p !== platform);
    for (const other of otherPlatforms) {
      const conditionalPattern = new RegExp(`{{#is${other.charAt(0).toUpperCase() + other.slice(1)}}}`, 'i');
      if (conditionalPattern.test(content)) {
        passed = false;
        results.issues.push({
          skill: skillFile,
          issue: `Contains unrendered conditional for ${other}`
        });
      }
    }
    
    // Should NOT contain closing conditional tags
    if (content.includes('{{/isClaude}}') || content.includes('{{/isCopilot}}') || content.includes('{{/isCodex}}')) {
      passed = false;
      results.issues.push({
        skill: skillFile,
        issue: 'Contains unrendered conditional closing tags'
      });
    }
    
    if (passed) {
      results.passed++;
    } else {
      results.failed++;
    }
  }
  
  console.log(`\n‚úÖ Clean rendering: ${results.passed}/${results.tested}`);
  if (results.failed > 0) {
    console.log(`‚ùå Issues found: ${results.failed}`);
    results.issues.forEach(i => {
      console.log(`  - ${i.skill}: ${i.issue}`);
    });
  }
  
  return results;
}

async function main() {
  console.log('üîß Testing Tool Mapping & Content Rendering\n');
  
  const platforms = ['copilot', 'claude', 'codex'];
  const allResults = [];
  
  for (const platform of platforms) {
    const mappingResults = await testToolMapping(platform);
    const renderingResults = await testContentRendering(platform);
    
    allResults.push({
      platform,
      toolMapping: mappingResults,
      contentRendering: renderingResults
    });
  }
  
  // Write results
  const resultsPath = path.join(__dirname, '..', 'test-environments', 'tool-mapping-results.json');
  fs.writeFileSync(resultsPath, JSON.stringify(allResults, null, 2));
  console.log(`\nüìÑ Results written to: test-environments/tool-mapping-results.json`);
  
  // Summary
  console.log('\n' + '='.repeat(60));
  console.log('Summary');
  console.log('='.repeat(60));
  
  let totalPassed = 0;
  let totalFailed = 0;
  
  for (const result of allResults) {
    const mappingRate = ((result.toolMapping.passed / result.toolMapping.tested) * 100).toFixed(0);
    const renderingRate = ((result.contentRendering.passed / result.contentRendering.tested) * 100).toFixed(0);
    
    console.log(`\n${result.platform}:`);
    console.log(`  Tool Mapping: ${mappingRate}% (${result.toolMapping.passed}/${result.toolMapping.tested})`);
    console.log(`  Content Rendering: ${renderingRate}% (${result.contentRendering.passed}/${result.contentRendering.tested})`);
    
    totalPassed += result.toolMapping.passed + result.contentRendering.passed;
    totalFailed += result.toolMapping.failed + result.contentRendering.failed;
  }
  
  console.log(`\n‚úÖ Overall: ${totalPassed}/${totalPassed + totalFailed} checks passed`);
  
  if (totalFailed > 0) {
    console.log(`\n‚ö†Ô∏è  ${totalFailed} checks failed - review results for details`);
    process.exit(1);
  }
  
  console.log('\n‚úÖ All tool mapping and content rendering checks passed!');
}

if (require.main === module) {
  main().catch(err => {
    console.error('‚ùå Test error:', err);
    process.exit(1);
  });
}

module.exports = { testToolMapping, testContentRendering, toolMappings };
```

**Make script executable:**

```bash
chmod +x scripts/test-tool-mapping.js
```
  </action>
  
  <verify>
```bash
# Verify script exists
ls -la scripts/test-tool-mapping.js

# Run tool mapping tests
node scripts/test-tool-mapping.js

# Verify results
cat test-environments/tool-mapping-results.json
```
  </verify>
  
  <done>
- [x] scripts/test-tool-mapping.js tests tool mapping across platforms (PLAT-05)
- [x] Script verifies Claude tools map to platform equivalents
- [x] Script tests platform-specific content renders correctly (PLAT-06)
- [x] Script sample-checks conditional rendering in generated files
- [x] Results written to tool-mapping-results.json
  </done>
</task>

<task name="test-platform-detection-and-legacy-fallback" type="auto">
  <files>
    scripts/test-platform-detection.js
  </files>
  
  <action>
Create script to test platform detection with fallbacks (PLAT-10), parallel installation (TEST-06), and legacy fallback (TEST-07).

**Create scripts/test-platform-detection.js:**

```javascript
#!/usr/bin/env node
const { execSync } = require('child_process');
const fs = require('fs');
const path = require('path');

async function testPlatformDetection() {
  console.log(`\n${'='.repeat(60)}`);
  console.log('Testing Platform Detection & Fallbacks');
  console.log('='.repeat(60));
  
  const testDir = path.join(__dirname, '..', 'test-environments', 'detection-test');
  
  // Create test environment without platform directories
  if (fs.existsSync(testDir)) {
    fs.rmSync(testDir, { recursive: true, force: true });
  }
  fs.mkdirSync(testDir, { recursive: true });
  
  // Copy repo to test dir
  const parentRepo = path.resolve(__dirname, '..');
  console.log('\nüì¶ Setting up detection test environment...');
  execSync(`git clone ${parentRepo} ${path.join(testDir, 'get-shit-done')}`, {
    stdio: 'pipe'
  });
  
  const repoDir = path.join(testDir, 'get-shit-done');
  
  console.log('\nüß™ Test 1: Platform detection when no platform available');
  
  try {
    // Run install without specifying platform
    const output = execSync('npm run install 2>&1 || true', {
      cwd: repoDir,
      encoding: 'utf8'
    });
    
    console.log(output);
    
    // Should gracefully handle absence of platform
    if (output.includes('No platform detected') || output.includes('Platform not available') || output.includes('Defaulting to')) {
      console.log('‚úÖ Graceful fallback message detected');
    } else {
      console.log('‚ö†Ô∏è  No explicit fallback message (may still work)');
    }
    
  } catch (error) {
    console.log('‚ö†Ô∏è  Installation without platform:', error.message);
  }
  
  console.log('\nüß™ Test 2: Install --all flag (parallel installation)');
  
  try {
    const startTime = Date.now();
    const output = execSync('npm run install -- --all 2>&1', {
      cwd: repoDir,
      encoding: 'utf8',
      timeout: 120000 // 2 minutes
    });
    const duration = ((Date.now() - startTime) / 1000).toFixed(1);
    
    console.log(output);
    console.log(`\n‚è±Ô∏è  Completed in ${duration}s`);
    
    // Verify all 3 platforms installed
    const platformDirs = [
      '.github/copilot/skills',
      '.claude/get-shit-done',
      '.codex/skills'
    ];
    
    let allInstalled = true;
    for (const dir of platformDirs) {
      const fullPath = path.join(repoDir, dir);
      if (fs.existsSync(fullPath)) {
        const skills = fs.readdirSync(fullPath).filter(f => f.startsWith('gsd-'));
        console.log(`‚úÖ ${dir}: ${skills.length} commands`);
      } else {
        console.log(`‚ùå ${dir}: not found`);
        allInstalled = false;
      }
    }
    
    if (allInstalled) {
      console.log('\n‚úÖ --all flag successfully installed all platforms');
    } else {
      console.log('\n‚ùå --all flag did not install all platforms');
    }
    
  } catch (error) {
    console.log('‚ùå --all flag test failed:', error.message);
  }
  
  console.log('\nüß™ Test 3: Legacy fallback when spec missing');
  
  // Rename a spec to test fallback
  const testSpecDir = path.join(repoDir, 'specs', 'skills', 'gsd-help');
  const backupDir = path.join(repoDir, 'specs', 'skills', 'gsd-help.backup');
  
  try {
    if (fs.existsSync(testSpecDir)) {
      fs.renameSync(testSpecDir, backupDir);
      console.log('  Temporarily removed gsd-help spec');
    }
    
    // Try to run installation
    const output = execSync('npm run install:claude 2>&1 || true', {
      cwd: repoDir,
      encoding: 'utf8'
    });
    
    // Should fallback to legacy command or skip gracefully
    if (output.includes('fallback') || output.includes('legacy') || output.includes('skipping')) {
      console.log('‚úÖ Legacy fallback detected');
    } else if (output.includes('Error') || output.includes('Failed')) {
      console.log('‚ùå Installation failed instead of falling back');
    } else {
      console.log('‚ö†Ô∏è  Unclear behavior - check output');
    }
    
    console.log(output.substring(0, 500)); // First 500 chars
    
  } catch (error) {
    console.log('‚ö†Ô∏è  Legacy fallback test:', error.message);
  } finally {
    // Restore spec
    if (fs.existsSync(backupDir)) {
      fs.renameSync(backupDir, testSpecDir);
      console.log('  Restored gsd-help spec');
    }
  }
  
  // Cleanup
  fs.rmSync(testDir, { recursive: true, force: true });
  console.log('\nüßπ Test environment cleaned up');
}

async function main() {
  console.log('üîç Platform Detection & Legacy Fallback Tests\n');
  
  await testPlatformDetection();
  
  console.log('\n' + '='.repeat(60));
  console.log('Platform Detection Tests Complete');
  console.log('='.repeat(60));
  console.log('\nNote: These tests verify graceful degradation and fallback behavior.');
  console.log('Review output above to confirm expected behavior.');
}

if (require.main === module) {
  main().catch(err => {
    console.error('‚ùå Test error:', err);
    process.exit(1);
  });
}

module.exports = { testPlatformDetection };
```

**Make script executable:**

```bash
chmod +x scripts/test-platform-detection.js
```
  </action>
  
  <verify>
```bash
# Verify script exists
ls -la scripts/test-platform-detection.js

# Run platform detection tests
node scripts/test-platform-detection.js

# Should test:
# 1. Platform detection when no platform available
# 2. --all flag parallel installation (TEST-06)
# 3. Legacy fallback when spec missing (TEST-07)
```
  </verify>
  
  <done>
- [x] scripts/test-platform-detection.js tests platform detection logic (PLAT-10)
- [x] Script tests graceful fallbacks when platform unavailable
- [x] Script tests npm run install --all parallel installation (TEST-06)
- [x] Script verifies concurrent execution across 3 platforms
- [x] Script tests legacy fallback when spec missing (TEST-07)
- [x] Script confirms backward compatibility maintained
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Platform testing scripts and installation verification across Copilot, Claude, and Codex platforms
  </what-built>
  
  <how-to-verify>
**Phase 1: Automated Installation Testing**

1. Set up test environments:
```bash
node test-environments/setup-test-env.js
```

2. Run automated installation tests:
```bash
node scripts/test-platform-install.js
```

Expected: All 3 platforms install 29 commands successfully

3. Review install-results.json:
```bash
cat test-environments/install-results.json
```

**Phase 2: Manual Command Testing**

For EACH platform (Copilot ‚Üí Claude ‚Üí Codex):

1. Navigate to test environment:
```bash
cd test-environments/copilot-test/get-shit-done  # (or claude-test, codex-test)
```

2. Run command testing guide:
```bash
node ../../scripts/test-platform-commands.js
```

3. Follow instructions to test command discovery and execution:
   - Verify command autocomplete works
   - Test 5-10 representative commands (help, progress, verify-installation, list-milestones, new-project)
   - Note any commands that don't appear or fail to execute

4. Document findings in test-results.json (create if doesn't exist):
```json
{
  "platform": "copilot",
  "commands_tested": 29,
  "commands_working": 27,
  "failures": [
    {
      "command": "gsd-example",
      "error": "Not discoverable in autocomplete",
      "severity": "P0"
    }
  ]
}
```

**Phase 3: Regression Testing**

5. Run regression tests (expanded to 15 commands):
```bash
node scripts/test-regression.js
```

Expected: All 15 safe commands have valid generated files across 3 platforms (45 tests total)

6. Review regression-results.json:
```bash
cat test-environments/regression-results.json
```

**Phase 4: Tool Mapping & Content Rendering**

7. Run tool mapping and content rendering tests:
```bash
node scripts/test-tool-mapping.js
```

Expected: Tools map correctly, conditionals fully rendered

8. Review tool-mapping-results.json:
```bash
cat test-environments/tool-mapping-results.json
```

**Phase 5: Platform Detection & Special Cases**

9. Run platform detection and fallback tests:
```bash
node scripts/test-platform-detection.js
```

Expected: Graceful fallbacks, --all flag works, legacy fallback functional

**Success criteria:**
- [ ] All 3 platforms: 29 commands installed (87 total)
- [ ] All 3 platforms: Commands discoverable via autocomplete
- [ ] All 3 platforms: Sample commands execute without errors
- [ ] Tool mapping: Claude tools correctly mapped to platform equivalents
- [ ] Content rendering: Platform-specific conditionals fully rendered
- [ ] Platform detection: Graceful fallbacks when platform unavailable
- [ ] Parallel install: npm run install --all works without conflicts
- [ ] Legacy fallback: System continues when spec missing
- [ ] Regression tests pass (15 commands √ó 3 platforms = 45 tests)
- [ ] Failures documented with severity (P0 = blocking, P1 = non-blocking)

**Failure handling:**
- If P0 failures: Note them, continue testing, batch for Phase 7.1 planning
- If P1 failures: Note them, continue testing, may defer to Phase 7.1 or 8
- Don't stop on first failure ‚Äî complete all testing for comprehensive view
  </how-to-verify>
  
  <resume-signal>
Type "approved" if testing complete and results documented, or describe issues found
  </resume-signal>
</task>

## Verification

```bash
# 1. Verify scripts created
ls -la scripts/test-*.js

# 2. Run automated installation test
node scripts/test-platform-install.js

# 3. Review installation results
cat test-environments/install-results.json

# 4. (MANUAL) Follow command testing guide
node scripts/test-platform-commands.js

# 5. Run regression tests
node scripts/test-regression.js

# 6. Review all results
cat test-environments/install-results.json
cat test-environments/regression-results.json
cat test-environments/test-results.json  # Created during manual testing
```

## Success Criteria

- [x] scripts/test-platform-install.js automates installation verification
- [x] scripts/test-platform-commands.js provides clear manual testing instructions
- [x] scripts/test-regression.js validates file structure across platforms
- [x] Automated tests run successfully and produce JSON results
- [x] Manual testing checkpoint provides clear verification steps
- [x] Test results documented in structured format (JSON files)
- [x] Failures categorized by severity (P0/P1) and type (Platform/Spec/Install/Test/Expected)

## Output

**Files created:**
- scripts/test-platform-install.js (~200 lines) - Automated installation testing
- scripts/test-platform-commands.js (~130 lines) - Manual testing guide
- scripts/test-regression.js (~170 lines) - File structure validation

**Files generated during testing:**
- test-environments/install-results.json - Installation test results
- test-environments/regression-results.json - Regression test results
- test-environments/test-results.json - Manual testing findings (human-created)

**Next:** Plan 3 analyzes test results, triages failures, generates validation report, and creates Phase 7.1 gap closure plan if needed
