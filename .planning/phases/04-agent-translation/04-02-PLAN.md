---
phase: 04-agent-translation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - bin/lib/orchestration/performance-tracker.js
  - .planning/metrics/agent-performance.json
autonomous: true

must_haves:
  truths:
    - "Agent execution time measured with sub-millisecond precision"
    - "Performance metrics stored to .planning/metrics/agent-performance.json"
    - "User can view average execution time per agent per CLI"
    - "Performance tracking doesn't block agent execution"
  artifacts:
    - path: "bin/lib/orchestration/performance-tracker.js"
      provides: "PerformanceTracker class using perf_hooks"
      min_lines: 100
      exports: ["PerformanceTracker"]
    - path: ".planning/metrics/agent-performance.json"
      provides: "Agent performance metrics storage"
      contains: "[]"
  key_links:
    - from: "bin/lib/orchestration/performance-tracker.js"
      to: "perf_hooks (built-in)"
      via: "require('perf_hooks')"
      pattern: "require.*perf_hooks"
    - from: "bin/lib/orchestration/performance-tracker.js"
      to: ".planning/metrics/agent-performance.json"
      via: "fs.promises.writeFile()"
      pattern: "writeFile.*metrics"
---

<objective>
Implement performance tracking for agent execution using Node.js built-in perf_hooks API to enable benchmarking across CLIs.

Purpose: Provide data-driven insights into agent execution time per CLI, enabling users to make informed decisions about which CLI to use for specific agents (AGENT-09 requirement).

Output: Performance tracker module that measures agent execution with sub-millisecond precision and stores metrics to .planning/metrics/ for analysis.
</objective>

<execution_context>
@.github/skills/get-shit-done/get-shit-done/workflows/execute-plan.md
@.github/skills/get-shit-done/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@.planning/phases/04-agent-translation/04-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create performance tracker with perf_hooks</name>
  <files>bin/lib/orchestration/performance-tracker.js</files>
  <action>
Create `bin/lib/orchestration/performance-tracker.js` implementing PerformanceTracker class:

1. **Import Node.js built-ins:**
   - `const { performance, PerformanceObserver } = require('perf_hooks');`
   - `const fs = require('fs').promises;`
   - `const path = require('path');`

2. **PerformanceTracker constructor:**
   - Accept `metricsFile` parameter (default: '.planning/metrics/agent-performance.json')
   - Initialize `this.metricsFile` and `this.metrics` Map
   - Create PerformanceObserver to automatically collect measures:
     ```javascript
     this.observer = new PerformanceObserver((items) => {
       items.getEntries().forEach(entry => {
         this._recordMetric(entry);
       });
     });
     this.observer.observe({ entryTypes: ['measure'] });
     ```

3. **startAgent(agentName, cli) method:**
   - Create mark name: `${agentName}:${cli}:start`
   - Call `performance.mark(markName)`
   - Return mark name for later use in endAgent()

4. **endAgent(agentName, cli, startMark) async method:**
   - Create end mark: `${agentName}:${cli}:end`
   - Create measure name: `${agentName}:${cli}`
   - Call `performance.mark(endMark)`
   - Call `performance.measure(measureName, startMark, endMark)`
   - Get measurement: `performance.getEntriesByName(measureName)`
   - Extract duration from last entry
   - Store metric via `_storeMetric(agentName, cli, duration)`
   - Cleanup marks and measures to prevent memory leak
   - Return duration

5. **_recordMetric(entry) method:**
   - Parse agent and cli from entry.name (format: "agent:cli")
   - Store in this.metrics Map with key `${agent}:${cli}`
   - Append to array: `{ duration: entry.duration, timestamp: Date.now() }`

6. **_storeMetric(agentName, cli, duration) async method:**
   - Create metric object: `{ agent, cli, duration, timestamp: ISO string }`
   - Ensure .planning/metrics/ directory exists via `fs.mkdir(path.dirname(metricsFile), {recursive: true})`
   - Read existing metrics from file (handle ENOENT gracefully)
   - Append new metric to array
   - Keep only last 100 measurements per agent/CLI combo (prevent unbounded growth)
   - Write back to file with `JSON.stringify(filtered, null, 2)`
   - Wrap entire method in try/catch, log warning on failure (don't throw — metric storage failure shouldn't break agent execution)

7. **getAverageTime(agentName, cli) method:**
   - Get metrics from this.metrics Map for key `${agentName}:${cli}`
   - Calculate average duration from array
   - Return null if no metrics exist

**Follow research Pattern 2 (performance-tracker.js lines 162-300).**

**Important:** Use async fs methods (fs.promises) to avoid blocking agent execution during metric storage.
  </action>
  <verify>
```bash
node -e "const {PerformanceTracker} = require('./bin/lib/orchestration/performance-tracker.js'); const t = new PerformanceTracker('.planning/metrics/test-perf.json'); const mark = t.startAgent('test-agent', 'claude'); setTimeout(() => { t.endAgent('test-agent', 'claude', mark).then(d => { console.log('Duration measured:', d, 'ms'); console.log('Average:', t.getAverageTime('test-agent', 'claude')); }); }, 100);"
```

Should output duration close to 100ms and average time.
  </verify>
  <done>
PerformanceTracker class exists with startAgent, endAgent, getAverageTime methods that use perf_hooks for sub-millisecond precision and store metrics to .planning/metrics/ asynchronously.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create metrics directory and initialize storage</name>
  <files>.planning/metrics/agent-performance.json</files>
  <action>
Create `.planning/metrics/` directory and initialize empty agent-performance.json:

1. **Ensure directory exists:**
   ```bash
   mkdir -p .planning/metrics
   ```

2. **Create initial JSON file:**
   ```bash
   echo '[]' > .planning/metrics/agent-performance.json
   ```

3. **Add .gitkeep to ensure directory tracked:**
   ```bash
   touch .planning/metrics/.gitkeep
   ```

**Why:** PerformanceTracker expects .planning/metrics/agent-performance.json to exist (though it handles missing file gracefully, creating it upfront avoids confusion).
  </action>
  <verify>
```bash
ls -la .planning/metrics/ && cat .planning/metrics/agent-performance.json
```

Should show agent-performance.json with `[]` content.
  </verify>
  <done>
.planning/metrics/ directory exists with agent-performance.json containing empty JSON array.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add performance tracking test</name>
  <files>bin/lib/orchestration/performance-tracker.test.js</files>
  <action>
Create `bin/lib/orchestration/performance-tracker.test.js` with simple console-based tests:

1. **Test 1: Tracker instantiation**
   - Create PerformanceTracker instance
   - Verify metricsFile set correctly
   - Assert metrics Map exists

2. **Test 2: Start and end tracking**
   - Call startAgent('test-agent', 'claude')
   - Simulate work with setTimeout(50ms)
   - Call endAgent() and verify duration close to 50ms (±10ms tolerance)

3. **Test 3: Average calculation**
   - Track same agent 3 times with different durations
   - Call getAverageTime()
   - Verify average is sum/3

4. **Test 4: Metric persistence**
   - Track agent
   - Read .planning/metrics/agent-performance.json directly
   - Verify metric written to file

**Use simple console assertions:**
```javascript
console.log('✅ Test passed') // or '❌ Test failed'
```

**Pattern from Phase 3:** See 03-03-SUMMARY.md for test structure (console-based, no framework).
  </action>
  <verify>
```bash
node bin/lib/orchestration/performance-tracker.test.js
```

Should output 4 test results with ✅ for each.
  </verify>
  <done>
Test file exists with 4 tests covering instantiation, tracking, average calculation, and persistence. All tests pass.
  </done>
</task>

</tasks>

<verification>
After completion:

1. **Precision verification:**
   ```bash
   node -e "const {PerformanceTracker} = require('./bin/lib/orchestration/performance-tracker.js'); const t = new PerformanceTracker(); const m = t.startAgent('test', 'claude'); setTimeout(() => t.endAgent('test', 'claude', m).then(d => console.log('Sub-ms precision:', d)), 10);"
   ```

2. **Storage verification:**
   ```bash
   cat .planning/metrics/agent-performance.json | jq 'length'
   ```

3. **Test suite:**
   ```bash
   node bin/lib/orchestration/performance-tracker.test.js
   ```
</verification>

<success_criteria>
1. PerformanceTracker uses perf_hooks for sub-millisecond timing
2. Metrics stored to .planning/metrics/agent-performance.json asynchronously
3. Storage failure doesn't break agent execution (try/catch with warning)
4. Last 100 measurements retained per agent/CLI combo (bounded growth)
5. getAverageTime() calculates correct average from tracked measurements
6. Test suite passes with 4/4 tests
</success_criteria>

<output>
After completion, create `.planning/phases/04-agent-translation/04-02-SUMMARY.md`
</output>
