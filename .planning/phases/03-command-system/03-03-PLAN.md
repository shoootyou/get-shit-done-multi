---
phase: 03-command-system
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - bin/lib/command-system/recorder.js
  - bin/lib/command-system/verifier.js
  - bin/test-command-system.js
autonomous: true

must_haves:
  truths:
    - "Command executions can be recorded with timestamp, CLI, command, args, and result"
    - "User can compare command results across different CLIs"
    - "Command system can be tested automatically"
    - "Installation verification confirms all commands are accessible"
  artifacts:
    - path: "bin/lib/command-system/recorder.js"
      provides: "Command execution recording to JSON"
      exports: ["recordExecution", "loadRecordings", "compareExecutions"]
      min_lines: 80
    - path: "bin/lib/command-system/verifier.js"
      provides: "Command system verification and validation"
      exports: ["verifyCommands", "verifyCommandAccessibility"]
      min_lines: 60
    - path: "bin/test-command-system.js"
      provides: "Automated test suite for command system"
      min_lines: 100
  key_links:
    - from: "bin/lib/command-system/recorder.js"
      to: ".planning/command-recordings/"
      via: "JSON file writes"
      pattern: "writeFile.*\\.json"
    - from: "bin/lib/command-system/verifier.js"
      to: "bin/lib/command-system/registry.js"
      via: "registry.list() verification"
      pattern: "registry\\.list\\("
    - from: "bin/test-command-system.js"
      to: "bin/lib/command-system/loader.js"
      via: "loadCommands() test"
      pattern: "loadCommands\\("
---

<objective>
Implement command recording for cross-CLI comparison and create verification tools for installation testing.

Purpose: Enable users to record command executions for documentation (CMD-06 requirement), compare results across CLIs (CMD-05 requirement), and verify post-installation that all commands are accessible (INSTALL-08 requirement).

Output: Recording system for command executions, verification tools, and automated test suite.
</objective>

<execution_context>
@.github/skills/get-shit-done/get-shit-done/workflows/execute-plan.md
@.github/skills/get-shit-done/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@.planning/phases/03-command-system/03-RESEARCH.md
@bin/lib/command-system/registry.js
@bin/lib/command-system/loader.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create command execution recorder</name>
  <files>bin/lib/command-system/recorder.js</files>
  <action>
    Create `bin/lib/command-system/recorder.js` to record command executions for CMD-05 and CMD-06 requirements.
    
    Components:
    1. recordExecution(commandName, args, result, cli, duration)
       - Create recording object: { timestamp: Date.now(), cli, command: commandName, args, result, duration, success: result.success }
       - Generate filename: `.planning/command-recordings/${cli}-${commandName}-${timestamp}.json`
       - Write to file using fs/promises.writeFile()
       - Create directory if it doesn't exist (mkdir with recursive: true)
       - Return recording object
    
    2. loadRecordings(commandName = null)
       - Read `.planning/command-recordings/*.json` files
       - Parse each JSON file
       - Filter by commandName if provided
       - Sort by timestamp descending (newest first)
       - Return array of recordings
    
    3. compareExecutions(commandName)
       - Load all recordings for commandName
       - Group by CLI
       - Compare results (success rate, duration, outputs)
       - Return comparison report: { command, byCLI: { 'claude-code': [...], 'copilot-cli': [...] }, summary: {...} }
    
    Use fs/promises for all file operations. Import path.join for cross-platform paths. Add JSDoc documentation.
    
    Research mentions JSON logs as recommendation - follow this approach.
  </action>
  <verify>
    ```bash
    # Test recording
    node -e "
    import('./bin/lib/command-system/recorder.js').then(m => {
      return m.recordExecution('test-command', ['arg1'], { success: true }, 'claude-code', 123);
    }).then(recording => {
      console.log('Recorded:', recording.command);
      console.log('File created:', recording.timestamp);
    });
    "
    
    # Verify file created
    ls .planning/command-recordings/*.json | head -1
    ```
    Should output: Recording confirmation and JSON file existence
  </verify>
  <done>
    - recorder.js exists with recordExecution()
    - Records executions to .planning/command-recordings/
    - JSON format includes timestamp, CLI, command, args, result, duration
    - loadRecordings() retrieves past executions
    - compareExecutions() groups recordings by CLI
    - Creates directory structure automatically
    - All functions have JSDoc documentation
  </done>
</task>

<task type="auto">
  <name>Task 2: Create command system verifier</name>
  <files>bin/lib/command-system/verifier.js</files>
  <action>
    Create `bin/lib/command-system/verifier.js` for INSTALL-08 requirement (post-install verification).
    
    Components:
    1. verifyCommands()
       - Load commands using loader.loadCommands()
       - Get all commands from registry.list()
       - Check each command has required metadata fields: name, description
       - Verify command count matches expected (24 commands)
       - Return { success: boolean, commandCount: number, issues: [] }
    
    2. verifyCommandAccessibility(cli)
       - Import detectCLI from '../cli-detector.js'
       - Verify current CLI matches expected
       - Check command files exist in appropriate directory:
         - Claude: ~/.claude/get-shit-done/commands/gsd/
         - Copilot: .github/skills/get-shit-done/commands/gsd/
         - Codex: .codex/skills/get-shit-done/commands/gsd/
       - Use Phase 1's getConfigPaths() for path resolution
       - Return { accessible: boolean, cli, path: string, fileCount: number }
    
    3. Import registry from './registry.js'
    4. Import loadCommands from './loader.js'
    5. Import getConfigPaths from '../path-utils.js' (Phase 1)
    
    Add JSDoc. Reference Phase 1's path-utils.js for consistent path handling.
  </action>
  <verify>
    ```bash
    # Test verification
    node -e "
    import('./bin/lib/command-system/verifier.js').then(m => {
      return m.verifyCommands();
    }).then(result => {
      console.log('Verification:', result.success ? 'PASS' : 'FAIL');
      console.log('Commands found:', result.commandCount);
      if (result.issues.length > 0) {
        console.log('Issues:', result.issues);
      }
    });
    "
    ```
    Should output: Verification PASS with 24 commands
  </verify>
  <done>
    - verifier.js exists with verifyCommands()
    - Checks all 24 commands loaded correctly
    - Validates required metadata fields present
    - verifyCommandAccessibility() checks CLI-specific paths
    - Uses Phase 1's getConfigPaths() for path resolution
    - Returns structured verification results
    - All functions documented with JSDoc
  </done>
</task>

<task type="auto">
  <name>Task 3: Create automated test suite</name>
  <files>bin/test-command-system.js</files>
  <action>
    Create `bin/test-command-system.js` as executable test suite for command system.
    
    Test structure:
    1. Test: Command Registry
       - Create registry, add command, retrieve command
       - Verify list() returns correct count
       - Verify has() returns true/false correctly
    
    2. Test: Argument Parser
       - Parse positionals: ['3'] → positionals: ['3']
       - Parse flags: ['--verbose'] → values: { verbose: true }
       - Parse combined: ['3', '--verbose', '--force'] → both
       - Parse with errors: invalid syntax → error field populated
    
    3. Test: Command Loader
       - Load from commands/gsd/
       - Verify 24 commands loaded
       - Check sample commands exist: help, execute-phase, new-project
    
    4. Test: Error Handler
       - Create CommandError with suggestions
       - Format error message (should include ❌ and suggestions)
       - Test degradeGracefully (should include ⚠️)
    
    5. Test: Help Generator
       - Generate full help (should list all commands grouped)
       - Generate specific help (should show command details)
    
    6. Test: Command Verifier
       - Run verifyCommands() (should pass with 24 commands)
    
    Format: Simple console.log() based assertions
    - ✅ Test name - on pass
    - ❌ Test name: error details - on fail
    - Exit with code 0 if all pass, 1 if any fail
    
    Add shebang: #!/usr/bin/env node
  </action>
  <verify>
    ```bash
    chmod +x bin/test-command-system.js
    node bin/test-command-system.js
    ```
    Should output: Series of ✅ marks and "All tests passed" message
  </verify>
  <done>
    - test-command-system.js exists and is executable
    - Tests registry operations (register, get, list, has)
    - Tests argument parsing with various inputs
    - Tests command loading from filesystem
    - Tests error formatting
    - Tests help generation
    - Tests verification functions
    - All tests pass with ✅ markers
    - Exits with appropriate code (0 = pass, 1 = fail)
  </done>
</task>

</tasks>

<verification>
Run complete command system verification:
```bash
# Run automated test suite
node bin/test-command-system.js

# Test recording functionality
node -e "
import('./bin/lib/command-system/loader.js').then(m => m.loadCommands('commands/gsd')).then(() => {
  import('./bin/lib/command-system/executor.js').then(e => {
    return e.executeCommand('gsd:help', []);
  }).then(result => {
    import('./bin/lib/command-system/recorder.js').then(r => {
      return r.recordExecution('gsd:help', [], result, 'test-cli', 50);
    });
  }).then(() => console.log('Recording test passed'));
});
"

# Test verification
node -e "
import('./bin/lib/command-system/verifier.js').then(m => {
  return m.verifyCommands();
}).then(result => {
  console.log('Command verification:', result.success ? 'PASS' : 'FAIL');
  console.log('Total commands:', result.commandCount);
});
"

# Verify recordings directory
ls -la .planning/command-recordings/ 2>/dev/null || echo "Recordings directory will be created on first use"
```

Expected: All tests pass, recording works, verification succeeds with 24 commands.
</verification>

<success_criteria>
1. Command executions can be recorded to `.planning/command-recordings/` with JSON format including timestamp, CLI, command, args, result, and duration
2. User can load and compare recordings across different CLIs to identify behavioral differences
3. Verification confirms all 24 commands are accessible and properly formatted
4. Automated test suite validates registry, parser, loader, error handler, help generator, and verifier
5. Post-install verification (INSTALL-08) checks command accessibility in CLI-specific directories
6. All tests pass with clear ✅ / ❌ indicators
7. Recording and verification satisfy requirements CMD-05, CMD-06, and INSTALL-08
</success_criteria>

<output>
After completion, create `.planning/phases/03-command-system/03-03-SUMMARY.md` following the template structure with:
- Frontmatter (phase, plan, subsystem, dependency graph, tech tracking)
- Summary of recording and verification capabilities
- Key decisions made during implementation
- Performance metrics (duration, completion timestamp)
</output>
