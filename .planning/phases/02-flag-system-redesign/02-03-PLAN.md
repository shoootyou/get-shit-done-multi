---
phase: 02-flag-system-redesign
plan: 03
type: execute
wave: 2
depends_on: [02-01]
files_modified:
  - bin/lib/flag-parser.test.js (new)
  - bin/lib/flag-validator.test.js (new)
  - bin/lib/old-flag-detector.test.js (new)
  - package.json
autonomous: true
must_haves:
  - All flag combinations tested (platform flags, scope modifiers, --all)
  - Old flag detection tested (single, multiple, mixed with new flags)
  - Validation tested (conflicting scopes, edge cases)
  - Deduplication tested (duplicate platforms)
  - Tests use Jest as established in project
---

# Phase 2, Plan 3: Flag System Test Suite

## Objective

Comprehensive unit tests for flag parsing, validation, and old flag detection modules. Ensures all flag combinations work correctly, edge cases are handled, and error conditions are caught.

## Context

**From Plan 1:** Three modules created:
- `bin/lib/old-flag-detector.js`
- `bin/lib/flag-parser.js`
- `bin/lib/flag-validator.js`

**Testing strategy (from ROADMAP):** Focus on unit/integration tests over E2E (less brittle, faster feedback)

**Test coverage target (from REQUIREMENTS TEST-01):** >80% coverage

**Existing test infrastructure:** Check if Jest is already configured (Phase 1 installed dependencies)

## Tasks

<task name="check-test-infrastructure" type="auto">
  <files>package.json</files>
  <action>
Check if Jest is configured, install if needed.

**Check package.json:**
```bash
grep -q "jest" package.json && echo "Jest found" || echo "Need to install Jest"
```

**If Jest not installed:**
```bash
npm install --save-dev jest@29
```

**Update package.json scripts:**
```json
"scripts": {
  "test": "jest",
  "test:watch": "jest --watch",
  "test:coverage": "jest --coverage"
}
```

**If Jest already installed:** Skip installation, just verify script exists.
  </action>
  <verify>
```bash
npm test -- --version 2>/dev/null || echo "Jest not configured"
```
  </verify>
  <done>
Jest is installed and test script configured in package.json
  </done>
</task>

<task name="test-old-flag-detector" type="auto">
  <files>bin/lib/old-flag-detector.test.js</files>
  <action>
Create comprehensive tests for old-flag-detector.js.

**Test cases:**

1. **Single old flag detection:**
   - `--local` detected and removed
   - `--global` detected and removed
   - `--codex-global` detected and removed
   - Warning message shown (verify console output)

2. **Multiple old flags:**
   - `--local --global` both detected
   - Single warning with both flags listed
   - Both removed from argv

3. **Mixed old/new flags:**
   - `--local --claude` → removes --local, keeps --claude
   - Warning shown, execution continues

4. **No old flags:**
   - `--claude --global` → no warning
   - Argv unchanged

5. **Old flag variations (should NOT match):**
   - `--locale` → not detected (different flag)
   - `-l` → not detected (short form is new flag)

**Test structure:**
```javascript
const detectAndFilterOldFlags = require('./old-flag-detector');

describe('Old Flag Detector', () => {
  let consoleWarnSpy;
  
  beforeEach(() => {
    consoleWarnSpy = jest.spyOn(console, 'warn').mockImplementation();
  });
  
  afterEach(() => {
    consoleWarnSpy.mockRestore();
  });
  
  test('detects and removes --local', () => {
    const cleaned = detectAndFilterOldFlags(['node', 'install.js', '--local']);
    expect(cleaned).toEqual(['node', 'install.js']);
    expect(consoleWarnSpy).toHaveBeenCalled();
    expect(consoleWarnSpy.mock.calls[0][0]).toContain('removed in v1.10.0');
  });
  
  // ... more tests
});
```

**Mock console.warn** to verify warning messages without polluting test output.
  </action>
  <verify>
```bash
npm test -- bin/lib/old-flag-detector.test.js
```
  </verify>
  <done>
Old flag detector tests exist, cover all detection cases, verify warnings shown
  </done>
</task>

<task name="test-flag-parser" type="auto">
  <files>bin/lib/flag-parser.test.js</files>
  <action>
Create comprehensive tests for flag-parser.js.

**Test cases:**

1. **Single platform flags:**
   - `--claude` → platforms: ['claude']
   - `--copilot` → platforms: ['copilot']
   - `--codex` → platforms: ['codex']

2. **Multiple platform flags:**
   - `--claude --copilot` → platforms: ['claude', 'copilot']
   - `--claude --copilot --codex` → platforms: ['claude', 'copilot', 'codex']

3. **--all flag:**
   - `--all` → platforms: ['claude', 'copilot', 'codex']
   - `--all --claude` → platforms: ['claude', 'copilot', 'codex'] (--all wins, info shown)

4. **Scope flags:**
   - `--global` → scope: 'global'
   - `--local` → scope: 'local'
   - `-g` → scope: 'global' (short form)
   - `-l` → scope: 'local' (short form)
   - No scope → scope: 'local' (default)

5. **Combined platform + scope:**
   - `--claude --global` → { platforms: ['claude'], scope: 'global' }
   - `--copilot --codex --local` → { platforms: ['copilot', 'codex'], scope: 'local' }

6. **Duplicate platforms:**
   - `--claude --claude` → platforms: ['claude'] (deduplicated)
   - Verify warning shown (spy on console.warn)

7. **No platforms (menu mode):**
   - No flags → { platforms: [], needsMenu: true }
   - `--global` only → { platforms: [], scope: 'global', needsMenu: true }

**Test structure:**
```javascript
const parseFlags = require('./flag-parser');

describe('Flag Parser', () => {
  test('parses single platform', () => {
    const result = parseFlags(['node', 'install.js', '--claude']);
    expect(result.platforms).toEqual(['claude']);
    expect(result.scope).toBe('local');
    expect(result.needsMenu).toBe(false);
  });
  
  test('--all includes all platforms', () => {
    const result = parseFlags(['node', 'install.js', '--all']);
    expect(result.platforms).toHaveLength(3);
    expect(result.platforms).toContain('claude');
    expect(result.platforms).toContain('copilot');
    expect(result.platforms).toContain('codex');
  });
  
  // ... more tests (aim for 20+ test cases)
});
```
  </action>
  <verify>
```bash
npm test -- bin/lib/flag-parser.test.js
```
  </verify>
  <done>
Flag parser tests exist, cover all combinations, verify deduplication warnings
  </done>
</task>

<task name="test-flag-validator" type="auto">
  <files>bin/lib/flag-validator.test.js</files>
  <action>
Create comprehensive tests for flag-validator.js.

**Test cases:**

1. **Conflicting scopes (should error):**
   - `--local --global` → exits with code 2
   - `-l -g` → exits with code 2
   - `--local -g` → exits with code 2

2. **Valid combinations (should pass):**
   - `--claude --global` → no error
   - `--all --local` → no error
   - Scope without platform → no error (triggers menu)

3. **Error message format:**
   - Verify error contains "Cannot use both"
   - Verify example shown
   - Verify exits with code 2

**Test structure:**
```javascript
const validateFlags = require('./flag-validator');

describe('Flag Validator', () => {
  let consoleErrorSpy;
  let processExitSpy;
  
  beforeEach(() => {
    consoleErrorSpy = jest.spyOn(console, 'error').mockImplementation();
    processExitSpy = jest.spyOn(process, 'exit').mockImplementation();
  });
  
  afterEach(() => {
    consoleErrorSpy.mockRestore();
    processExitSpy.mockRestore();
  });
  
  test('rejects conflicting scopes --local --global', () => {
    const argv = ['node', 'install.js', '--local', '--global'];
    const config = { platforms: ['claude'], scope: 'local' };
    
    validateFlags(argv, config);
    
    expect(processExitSpy).toHaveBeenCalledWith(2);
    expect(consoleErrorSpy.mock.calls[0][0]).toContain('Cannot use both');
  });
  
  test('allows --claude --global', () => {
    const argv = ['node', 'install.js', '--claude', '--global'];
    const config = { platforms: ['claude'], scope: 'global' };
    
    expect(() => validateFlags(argv, config)).not.toThrow();
    expect(processExitSpy).not.toHaveBeenCalled();
  });
  
  // ... more tests
});
```

**Mock process.exit** to prevent tests from actually exiting.
  </action>
  <verify>
```bash
npm test -- bin/lib/flag-validator.test.js
```
  </verify>
  <done>
Flag validator tests exist, verify error conditions and exit codes
  </done>
</task>

<task name="run-coverage-report" type="auto">
  <files>coverage/</files>
  <action>
Run test suite with coverage report.

```bash
cd /workspace
npm test -- --coverage bin/lib/old-flag-detector.test.js bin/lib/flag-parser.test.js bin/lib/flag-validator.test.js
```

**Verify coverage:**
- old-flag-detector.js: >80%
- flag-parser.js: >80%
- flag-validator.js: >80%

**If coverage below 80%:**
- Identify uncovered lines in coverage report
- Add tests for uncovered edge cases
- Re-run until >80%

**Coverage report location:** Displayed in terminal, or in `coverage/` directory
  </action>
  <verify>
```bash
npm test -- --coverage --collectCoverageFrom='bin/lib/old-flag-detector.js' --collectCoverageFrom='bin/lib/flag-parser.js' --collectCoverageFrom='bin/lib/flag-validator.js'
```
  </verify>
  <done>
Test coverage >80% for all three modules, coverage report generated
  </done>
</task>

## Verification

After all tasks complete:

```bash
# Run full test suite
npm test

# Verify specific modules
npm test -- bin/lib/old-flag-detector.test.js
npm test -- bin/lib/flag-parser.test.js
npm test -- bin/lib/flag-validator.test.js

# Check coverage
npm test -- --coverage
```

## Success Criteria

- ✅ Jest installed and configured
- ✅ Old flag detector tests cover: detection, removal, warnings, edge cases
- ✅ Flag parser tests cover: all platform combinations, scope modifiers, --all, deduplication
- ✅ Flag validator tests cover: conflicting scopes, error messages, exit codes
- ✅ All tests pass
- ✅ Coverage >80% for all three modules
- ✅ Tests are clear, well-organized, and maintainable

## Output

**Files created:**
- `bin/lib/old-flag-detector.test.js` - 10+ test cases
- `bin/lib/flag-parser.test.js` - 20+ test cases
- `bin/lib/flag-validator.test.js` - 8+ test cases

**Files modified:**
- `package.json` - Jest configuration and test scripts (if needed)

**Test coverage:**
- old-flag-detector.js: >80%
- flag-parser.js: >80%
- flag-validator.js: >80%

**Ready for:**
- Phase 3: Interactive menu (tests establish pattern)
- Phase 7: Full integration testing (builds on unit tests)
