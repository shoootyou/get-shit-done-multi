---
phase: 01-core-installer
plan: 03
type: execute
wave: 3
depends_on: ["01-01", "01-02"]
files_modified:
  - tests/integration/install.test.js
  - tests/integration/template-rendering.test.js
  - tests/integration/error-scenarios.test.js
  - tests/helpers/test-env.js
  - package.json (add test script and test dependencies)
autonomous: false
must_haves:
  observable_truths:
    - Integration tests verify complete installation flow
    - Tests verify all 29 skills and 13 agents install correctly
    - Tests verify template variables render correctly
    - Tests verify error scenarios (permissions, disk space, invalid flags)
    - Tests run in isolated /tmp directories (never in source)
    - All Phase 1 success criteria are tested and passing
  required_artifacts:
    - tests/integration/install.test.js (complete installation flow tests)
    - tests/integration/template-rendering.test.js (variable substitution tests)
    - tests/integration/error-scenarios.test.js (error handling tests)
    - tests/helpers/test-env.js (creates isolated /tmp test environments)
  required_wiring:
    - Tests use test-env.js helper to create isolated /tmp directories
    - Tests verify actual file system state after installation
    - Tests verify variable substitution in installed files
    - Tests verify error messages for common failures
    - Tests run via npm test command
  key_links:
    - package.json includes test script and testing library
    - All tests execute in /tmp (never modify source)
    - Tests verify all Phase 1 requirements are met
---

# Phase 1, Plan 3: Integration & Verification

## Objective

Create comprehensive integration tests that verify the complete installation flow works end-to-end. Test that all 29 skills and 13 agents install correctly, template variables render properly, error scenarios are handled gracefully, and all Phase 1 requirements are satisfied. Ensure all tests run in isolated /tmp directories per TEST-01 requirement.

## Context

**Why this matters:** Integration tests are the only way to verify the installer actually works. Unit tests for individual modules (from Plan 1) aren't enough - we need to test the complete orchestration flow. This is also a checkpoint task where the human verifies the installation works correctly before marking Phase 1 complete.

**What exists:**
- From Plan 1: All foundation modules (file-operations, path-resolver, template-renderer, etc.)
- From Plan 2: CLI entry point, installer orchestrator, platform detector, templates/
- From repository: .github/skills/ with 29 skills, .github/agents/ with 13 agents

**Decisions from context:**
- ALL tests execute in /tmp directory only (from CONTEXT.md TEST-01 requirement)
- Each test gets unique isolated folder /tmp/gsd-test-{timestamp}/ (from CONTEXT.md)
- Tests verify actual filesystem state, not just exit codes (integration testing)
- Tests clean up after success, may leave failed tests for debugging (from CONTEXT.md TEST-02)
- Human verification checkpoint after tests pass (checkpoint:human-verify)

**Requirements covered:**
- TEST-01: Testing isolation (all tests in /tmp)
- TEST-02: Test cleanup
- All Phase 1 success criteria from ROADMAP.md (verified via tests)

**Requirements verified by tests:**
- INSTALL-01: NPX entry point works
- INSTALL-02: File system operations work
- INSTALL-03: Template rendering works
- CLI-02: --claude flag works
- CLI-05: --help and --version work
- SAFETY-02: Path normalization works
- TEMPLATE-01: Templates install correctly
- TEMPLATE-01B: Variable conversion works
- TEMPLATE-03: All template variables render

**Dependencies:**
- Plans 1 and 2 must complete (need full installer implementation)

## Tasks

<task name="add-test-dependencies" type="auto">
  <files>package.json</files>
  <action>
Add testing dependencies and test script to package.json:

```bash
# Install testing libraries
npm install --save-dev vitest @vitest/ui
```

Add test script to package.json:
```json
{
  "scripts": {
    "test": "vitest run",
    "test:watch": "vitest watch",
    "test:ui": "vitest --ui"
  }
}
```

**Why Vitest:**
- Native ESM support (our project uses type: "module")
- Fast execution with parallel test running
- Compatible API with Jest (familiar for most developers)
- Built-in coverage support
- Small bundle size

Alternative considered: Node's built-in test runner (node:test). Vitest chosen for better assertion library and parallel execution.
  </action>
  <verify>
```bash
# Verify dependencies installed
npm ls vitest @vitest/ui

# Verify test script exists
cat package.json | grep '"test":'
```
  </verify>
  <done>package.json includes vitest dependencies and test scripts</done>
</task>

<task name="create-test-environment-helper" type="auto">
  <files>tests/helpers/test-env.js</files>
  <action>
Create test environment helper that creates isolated /tmp directories for testing.

**Purpose:** Enforce TEST-01 requirement that all tests execute in /tmp, never in source directory.

**Implementation:**
```javascript
import { mkdtempSync, rmSync, mkdirSync, cpSync } from 'fs';
import { tmpdir } from 'os';
import { join } from 'path';
import { execSync } from 'child_process';

/**
 * Creates isolated test environment in /tmp
 * Returns { testDir, cleanup, installCmd }
 */
export function createTestEnv() {
  // Create unique /tmp directory
  const testDir = mkdtempSync(join(tmpdir(), 'gsd-test-'));
  
  // Get source directory (where package.json lives)
  const sourceDir = join(import.meta.dirname, '../..');
  
  return {
    testDir,
    sourceDir,
    
    /**
     * Run installer command in test directory
     * Example: installCmd('--claude --local')
     */
    installCmd(args = '') {
      const cmd = `node "${join(sourceDir, 'bin/install.js')}" ${args}`;
      return execSync(cmd, { 
        cwd: testDir,
        encoding: 'utf-8',
        env: { ...process.env, FORCE_COLOR: '0' } // Disable colors in tests
      });
    },
    
    /**
     * Copy templates to test directory (for testing build script)
     */
    copyTemplates() {
      const templatesDir = join(sourceDir, 'templates');
      const destDir = join(testDir, 'templates');
      cpSync(templatesDir, destDir, { recursive: true });
    },
    
    /**
     * Copy source files to test directory (for testing build script)
     */
    copySource() {
      const githubDir = join(sourceDir, '.github');
      const gsdDir = join(sourceDir, 'get-shit-done');
      
      cpSync(githubDir, join(testDir, '.github'), { recursive: true });
      cpSync(gsdDir, join(testDir, 'get-shit-done'), { recursive: true });
    },
    
    /**
     * Cleanup test directory (called after successful test)
     */
    cleanup() {
      try {
        rmSync(testDir, { recursive: true, force: true });
      } catch (error) {
        // Ignore cleanup errors (failed tests may want to inspect directory)
        console.warn(`Warning: Failed to cleanup ${testDir}`);
      }
    }
  };
}

/**
 * Creates isolated test environment and auto-cleans up
 * Use in tests: const env = await withTestEnv(async (env) => { ... })
 */
export async function withTestEnv(testFn) {
  const env = createTestEnv();
  try {
    await testFn(env);
    env.cleanup();
  } catch (error) {
    // Don't cleanup on error - allow inspection
    console.error(`Test failed. Inspect: ${env.testDir}`);
    throw error;
  }
}
```

Export both createTestEnv (manual control) and withTestEnv (auto-cleanup) for flexibility.
  </action>
  <verify>
```bash
# Test helper can create and cleanup test environments
node -e "
import { createTestEnv } from './tests/helpers/test-env.js';
import { existsSync } from 'fs';

const env = createTestEnv();
console.assert(existsSync(env.testDir), 'Test dir not created');
console.assert(env.testDir.includes('/tmp/gsd-test-'), 'Test dir not in /tmp');

env.cleanup();
console.assert(!existsSync(env.testDir), 'Test dir not cleaned up');

console.log('✓ Test environment helper works');
"
```
  </verify>
  <done>tests/helpers/test-env.js exists with createTestEnv and withTestEnv functions</done>
</task>

<task name="create-installation-integration-tests" type="auto">
  <files>tests/integration/install.test.js</files>
  <action>
Create integration tests for complete installation flow.

**Test suites:**

1. **Basic Installation Flow**
   - Test: Install to local directory (./.claude/)
   - Test: Install to global directory (~/.claude/) (in test environment)
   - Test: Skills directory created with correct structure
   - Test: Agents directory created with correct structure
   - Test: Shared directory copied

2. **Skill Installation**
   - Test: All 29 skills install (count directories)
   - Test: Each skill has SKILL.md file
   - Test: Skills have gsd-* naming convention

3. **Agent Installation**
   - Test: All 13 agents install (count files)
   - Test: Agents have .agent.md extension
   - Test: Agents have gsd-* naming convention

4. **CLI Flags**
   - Test: --help shows usage information
   - Test: --version shows installer version
   - Test: --claude flag required (error without it)
   - Test: --local flag installs to ./.claude/
   - Test: Default (no --local) installs to ~/.claude/
   - Test: --no-color disables colors

**Implementation:**
```javascript
import { describe, it, expect } from 'vitest';
import { withTestEnv } from '../helpers/test-env.js';
import { readdirSync, existsSync, readFileSync } from 'fs';
import { join } from 'path';

describe('Installation Flow', () => {
  it('installs to local directory with --local flag', async () => {
    await withTestEnv(async (env) => {
      const output = env.installCmd('--claude --local');
      
      // Verify success message
      expect(output).toContain('Installation complete');
      
      // Verify directories created
      expect(existsSync(join(env.testDir, '.claude/skills/gsd'))).toBe(true);
      expect(existsSync(join(env.testDir, '.claude/agents'))).toBe(true);
      expect(existsSync(join(env.testDir, '.claude/get-shit-done'))).toBe(true);
    });
  });
  
  it('installs all 29 skills', async () => {
    await withTestEnv(async (env) => {
      env.installCmd('--claude --local');
      
      const skillsDir = join(env.testDir, '.claude/skills/gsd');
      const skills = readdirSync(skillsDir, { withFileTypes: true })
        .filter(e => e.isDirectory() && e.name.startsWith('gsd-'));
      
      expect(skills.length).toBe(29);
      
      // Verify each skill has SKILL.md
      skills.forEach(skill => {
        const skillFile = join(skillsDir, skill.name, 'SKILL.md');
        expect(existsSync(skillFile)).toBe(true);
      });
    });
  });
  
  it('installs all 13 agents', async () => {
    await withTestEnv(async (env) => {
      env.installCmd('--claude --local');
      
      const agentsDir = join(env.testDir, '.claude/agents');
      const agents = readdirSync(agentsDir)
        .filter(f => f.endsWith('.agent.md') && f.startsWith('gsd-'));
      
      expect(agents.length).toBe(13);
    });
  });
});

describe('CLI Flags', () => {
  it('shows help with --help', async () => {
    await withTestEnv(async (env) => {
      const output = env.installCmd('--help');
      expect(output).toContain('Install to Claude Code');
      expect(output).toContain('Examples:');
    });
  });
  
  it('shows version with --version', async () => {
    await withTestEnv(async (env) => {
      const output = env.installCmd('--version');
      expect(output).toContain('get-shit-done-multi v');
      expect(output).toMatch(/v\d+\.\d+\.\d+/); // Version format
    });
  });
  
  it('requires --claude flag', async () => {
    await withTestEnv(async (env) => {
      try {
        env.installCmd(''); // No flags
        expect.fail('Should have thrown error');
      } catch (error) {
        expect(error.message).toContain('requires --claude flag');
      }
    });
  });
});

describe('Overwrite Behavior', () => {
  it('overwrites existing installation silently', async () => {
    await withTestEnv(async (env) => {
      // First installation
      env.installCmd('--claude --local');
      
      // Modify installed file
      const testFile = join(env.testDir, '.claude/skills/gsd/gsd-new-project/SKILL.md');
      const original = readFileSync(testFile, 'utf-8');
      const modified = original + '\n<!-- MODIFIED -->';
      require('fs').writeFileSync(testFile, modified);
      
      // Second installation (overwrite)
      const output = env.installCmd('--claude --local');
      
      expect(output).toContain('Existing installation found');
      expect(output).toContain('Files will be overwritten');
      
      // Verify file was overwritten (no MODIFIED marker)
      const afterOverwrite = readFileSync(testFile, 'utf-8');
      expect(afterOverwrite).not.toContain('MODIFIED');
    });
  });
});
```

Add more test cases for:
- Symlink warnings
- Next steps in success message
- Progress messages during installation
  </action>
  <verify>
```bash
# Run integration tests
npm test tests/integration/install.test.js

# Verify tests pass
echo "✓ Integration tests created"
```
  </verify>
  <done>tests/integration/install.test.js exists with comprehensive installation flow tests</done>
</task>

<task name="create-template-rendering-tests" type="auto">
  <files>tests/integration/template-rendering.test.js</files>
  <action>
Create integration tests for template variable rendering.

**Test suites:**

1. **Variable Substitution**
   - Test: {{PLATFORM_ROOT}} → .claude/
   - Test: {{VERSION}} → 2.0.0
   - Test: {{COMMAND_PREFIX}} → /gsd-
   - Test: {{INSTALL_DATE}} → ISO 8601 timestamp
   - Test: {{USER}} → current username
   - Test: {{PLATFORM_NAME}} → claude

2. **Variable Validation**
   - Test: Lowercase variables remain unchanged ({{platform}} stays as-is)
   - Test: Unknown variables → [MISSING:VAR_NAME]
   - Test: No {{VARIABLES}} remain after rendering

3. **Template Files**
   - Test: Skills have variables substituted
   - Test: Agents have variables substituted
   - Test: Shared directory files copied unchanged (no variables expected)

**Implementation:**
```javascript
import { describe, it, expect } from 'vitest';
import { withTestEnv } from '../helpers/test-env.js';
import { readFileSync } from 'fs';
import { join } from 'path';

describe('Template Variable Rendering', () => {
  it('replaces PLATFORM_ROOT with .claude/', async () => {
    await withTestEnv(async (env) => {
      env.installCmd('--claude --local');
      
      // Find a skill file and check content
      const skillFile = join(env.testDir, '.claude/skills/gsd/gsd-new-project/SKILL.md');
      const content = readFileSync(skillFile, 'utf-8');
      
      expect(content).toContain('.claude/');
      expect(content).not.toContain('{{PLATFORM_ROOT}}');
    });
  });
  
  it('replaces VERSION with 2.0.0', async () => {
    await withTestEnv(async (env) => {
      env.installCmd('--claude --local');
      
      const skillFile = join(env.testDir, '.claude/skills/gsd/gsd-new-project/SKILL.md');
      const content = readFileSync(skillFile, 'utf-8');
      
      // VERSION may appear in skill descriptions
      if (content.includes('{{VERSION}}')) {
        expect.fail('{{VERSION}} not replaced');
      }
    });
  });
  
  it('replaces COMMAND_PREFIX with /gsd-', async () => {
    await withTestEnv(async (env) => {
      env.installCmd('--claude --local');
      
      const skillFile = join(env.testDir, '.claude/skills/gsd/gsd-new-project/SKILL.md');
      const content = readFileSync(skillFile, 'utf-8');
      
      expect(content).toContain('/gsd-');
      expect(content).not.toContain('{{COMMAND_PREFIX}}');
    });
  });
  
  it('replaces INSTALL_DATE with ISO 8601 timestamp', async () => {
    await withTestEnv(async (env) => {
      env.installCmd('--claude --local');
      
      // Check if any file has INSTALL_DATE (may not be in all templates)
      // If present, verify it's a valid ISO timestamp
      const skillFile = join(env.testDir, '.claude/skills/gsd/gsd-new-project/SKILL.md');
      const content = readFileSync(skillFile, 'utf-8');
      
      expect(content).not.toContain('{{INSTALL_DATE}}');
      
      // If timestamp present, verify format
      const isoMatch = content.match(/\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}/);
      if (isoMatch) {
        const date = new Date(isoMatch[0]);
        expect(date).toBeInstanceOf(Date);
        expect(date.toString()).not.toBe('Invalid Date');
      }
    });
  });
  
  it('leaves no unsubstituted uppercase variables', async () => {
    await withTestEnv(async (env) => {
      env.installCmd('--claude --local');
      
      const skillFile = join(env.testDir, '.claude/skills/gsd/gsd-new-project/SKILL.md');
      const content = readFileSync(skillFile, 'utf-8');
      
      // Check for any remaining {{UPPERCASE}} patterns
      const unreplacedVars = content.match(/\{\{[A-Z_]+\}\}/g);
      if (unreplacedVars) {
        expect.fail(`Unreplaced variables found: ${unreplacedVars.join(', ')}`);
      }
    });
  });
});

describe('Variable Validation', () => {
  it('handles missing variables with [MISSING:VAR_NAME]', async () => {
    // This test requires injecting a template with unknown variable
    // Skip for now - validate in template-renderer unit tests instead
  });
});
```
  </action>
  <verify>
```bash
# Run template rendering tests
npm test tests/integration/template-rendering.test.js

echo "✓ Template rendering tests created"
```
  </verify>
  <done>tests/integration/template-rendering.test.js exists with variable rendering verification tests</done>
</task>

<task name="create-error-scenario-tests" type="auto">
  <files>tests/integration/error-scenarios.test.js</files>
  <action>
Create integration tests for error handling scenarios.

**Test suites:**

1. **Permission Errors**
   - Test: EACCES error shows actionable fix message
   - Test: Permission error includes path in message
   - Test: Error suggests checking permissions

2. **Invalid Flags**
   - Test: Unknown flag shows suggestion
   - Test: Misspelled flag suggests correct flag
   - Test: --copilot in Phase 1 shows "coming in Phase 2" message

3. **Missing Requirements**
   - Test: Missing --claude flag shows error
   - Test: Error message is user-friendly (not stack trace)

4. **Output Formatting**
   - Test: --no-color disables colored output
   - Test: Success message includes next steps
   - Test: Progress messages show checkmarks

**Implementation:**
```javascript
import { describe, it, expect } from 'vitest';
import { withTestEnv } from '../helpers/test-env.js';
import { join } from 'path';
import { chmodSync } from 'fs';

describe('Error Scenarios', () => {
  it('shows actionable error for permission denied', async () => {
    await withTestEnv(async (env) => {
      // Create read-only directory
      const roDir = join(env.testDir, '.claude');
      require('fs').mkdirSync(roDir);
      chmodSync(roDir, 0o444); // Read-only
      
      try {
        env.installCmd('--claude --local');
        expect.fail('Should have thrown permission error');
      } catch (error) {
        const output = error.message;
        expect(output).toContain('Permission denied');
        expect(output).toContain('Fix:');
      } finally {
        // Restore permissions for cleanup
        chmodSync(roDir, 0o755);
      }
    });
  });
  
  it('shows "coming in Phase 2" for --copilot', async () => {
    await withTestEnv(async (env) => {
      try {
        env.installCmd('--copilot');
        expect.fail('Should have thrown unsupported platform error');
      } catch (error) {
        expect(error.message).toContain('Phase 1');
        expect(error.message).toContain('Phase 2');
      }
    });
  });
  
  it('requires --claude flag', async () => {
    await withTestEnv(async (env) => {
      try {
        env.installCmd(''); // No platform flag
        expect.fail('Should have thrown missing flag error');
      } catch (error) {
        expect(error.message).toContain('--claude');
      }
    });
  });
});

describe('Output Formatting', () => {
  it('disables colors with --no-color', async () => {
    await withTestEnv(async (env) => {
      const output = env.installCmd('--claude --local --no-color');
      
      // Check that ANSI color codes are NOT present
      expect(output).not.toMatch(/\u001b\[\d+m/); // ANSI escape codes
      expect(output).toContain('Installation complete');
    });
  });
  
  it('includes next steps in success message', async () => {
    await withTestEnv(async (env) => {
      const output = env.installCmd('--claude --local');
      
      expect(output).toContain('Next steps:');
      expect(output).toContain('/gsd-new-project');
      expect(output).toContain('Restart Claude Code');
    });
  });
  
  it('shows progress messages', async () => {
    await withTestEnv(async (env) => {
      const output = env.installCmd('--claude --local');
      
      expect(output).toContain('Validating templates');
      expect(output).toContain('Installing skills');
      expect(output).toContain('Installing agents');
    });
  });
});

describe('Invalid Flags', () => {
  it('suggests correct flag for typos', async () => {
    await withTestEnv(async (env) => {
      try {
        env.installCmd('--clude'); // Typo of --claude
        expect.fail('Should have thrown unknown option error');
      } catch (error) {
        // Commander provides suggestions automatically
        expect(error.message).toMatch(/unknown option|--claude/i);
      }
    });
  });
});
```
  </action>
  <verify>
```bash
# Run error scenario tests
npm test tests/integration/error-scenarios.test.js

echo "✓ Error scenario tests created"
```
  </verify>
  <done>tests/integration/error-scenarios.test.js exists with error handling verification tests</done>
</task>

<task name="run-full-test-suite" type="auto">
  <files>package.json (test script)</files>
  <action>
Run complete test suite to verify all Phase 1 functionality:

```bash
# Run all integration tests
npm test

# Verify test results
# - All tests should pass
# - Tests should run in /tmp (verify no source file modifications)
# - Tests should cleanup after success
```

**Expected test output:**
```
✓ tests/integration/install.test.js (X tests)
✓ tests/integration/template-rendering.test.js (X tests)
✓ tests/integration/error-scenarios.test.js (X tests)

Test Files  3 passed (3)
     Tests  XX passed (XX)
```

If tests fail:
1. Check error messages for specific failures
2. Inspect /tmp/gsd-test-* directories (failed tests leave them for debugging)
3. Fix issues in implementation modules
4. Re-run tests

**Verify no source file modifications:**
```bash
git status --porcelain .github/ get-shit-done/
```

Expected: No output (no modifications)

If source files modified:
- Tests are NOT following TEST-01 requirement
- Fix test-env.js to ensure isolation
- Re-run tests after fix
  </action>
  <verify>
```bash
# Run full test suite
npm test 2>&1 | tee test-results.txt

# Check for passing tests
grep -q "Test Files.*passed" test-results.txt && echo "✓ Tests pass"

# Verify source files unchanged (critical TEST-01 requirement)
if [ -n "$(git status --porcelain .github/ get-shit-done/)" ]; then
  echo "✗ CRITICAL: Source files modified during testing"
  git status .github/ get-shit-done/
  exit 1
else
  echo "✓ Source files unchanged (TEST-01 verified)"
fi

# Cleanup test results
rm test-results.txt
```
  </verify>
  <done>Full test suite runs and passes, all Phase 1 functionality verified, source files remain unchanged</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete Phase 1 installer with:
- Foundation modules (file operations, path resolution, template rendering, validation, CLI output, error formatting)
- Build script that generates templates/ from .github/ source
- CLI entry point with Commander (--claude, --help, --version flags)
- Installer orchestrator that coordinates validation → rendering → copying → reporting
- Platform detector that finds existing GSD installations
- Pre-built templates/ directory with 29 skills and 13 agents
- Comprehensive integration tests verifying all functionality

Installation flow:
1. User runs: `npx get-shit-done-multi --claude`
2. Installer validates templates (pre-flight check)
3. Installer detects existing installation (if any) and warns
4. Installer renders template variables ({{PLATFORM_ROOT}} → .claude/, etc.)
5. Installer copies 29 skills to ~/.claude/skills/gsd/
6. Installer copies 13 agents to ~/.claude/agents/
7. Installer copies shared directory to ~/.claude/get-shit-done/
8. Installer shows success message with next steps
  </what-built>
  
  <how-to-verify>
Verify the installer works correctly before marking Phase 1 complete.

**1. Test installation in isolated environment:**
```bash
# Create test directory (per TEST-01 requirement)
TEST_DIR="/tmp/gsd-manual-test-$(date +%s)"
mkdir -p "$TEST_DIR"
cd "$TEST_DIR"

# Run installer (local installation for testing)
node /path/to/source/bin/install.js --claude --local

# Expected output:
# ✓ Validating templates...
# ✓ Creating directories...
# ✓ Installing skills...
# ✓ Installing agents...
# ✓ Installing shared resources...
# ✓ Installation complete!
# 
# Installed to: ./.claude/
# 
# Next steps:
# - Restart Claude Code or reload skills
# - Run /gsd-new-project to start a new project
```

**2. Verify installed structure:**
```bash
# Check directories created
ls -la .claude/skills/gsd/
ls -la .claude/agents/
ls -la .claude/get-shit-done/

# Count skills (should be 29)
find .claude/skills/gsd -type d -name "gsd-*" | wc -l

# Count agents (should be 13)
find .claude/agents -name "*.agent.md" | wc -l

# Verify skills have SKILL.md files
find .claude/skills/gsd -name "SKILL.md" | wc -l
```

**3. Verify template variables replaced:**
```bash
# Check a sample skill file
cat .claude/skills/gsd/gsd-new-project/SKILL.md

# Expected: Should contain .claude/ (not {{PLATFORM_ROOT}})
# Expected: Should contain /gsd- commands (not {{COMMAND_PREFIX}})
# Expected: Should NOT contain any {{UPPERCASE}} variables

# Check for unreplaced variables
grep -r "{{[A-Z_]*}}" .claude/ && echo "✗ Unreplaced variables found" || echo "✓ All variables replaced"
```

**4. Test CLI flags:**
```bash
# Test --help
node /path/to/source/bin/install.js --help
# Expected: Shows usage information and examples

# Test --version
node /path/to/source/bin/install.js --version
# Expected: Shows version (2.0.0) and detected installations

# Test invalid flag
node /path/to/source/bin/install.js --clude
# Expected: Shows "unknown option" and suggests "--claude"
```

**5. Test re-installation (overwrite behavior):**
```bash
# Modify an installed file
echo "<!-- MODIFIED -->" >> .claude/skills/gsd/gsd-new-project/SKILL.md

# Re-run installer
node /path/to/source/bin/install.js --claude --local

# Expected: Shows "Existing installation found" warning
# Expected: "Files will be overwritten"

# Verify file was overwritten (MODIFIED marker gone)
grep "MODIFIED" .claude/skills/gsd/gsd-new-project/SKILL.md && echo "✗ File not overwritten" || echo "✓ File overwritten"
```

**6. Verify source files unchanged:**
```bash
cd /path/to/source
git status --porcelain .github/ get-shit-done/

# Expected: No output (no modifications)
# Critical: Source files should NEVER be modified during installation or testing
```

**7. Cleanup test directory:**
```bash
rm -rf "$TEST_DIR"
```

**Success criteria checklist:**
- [ ] Installation completes successfully (no errors)
- [ ] 29 skills installed to .claude/skills/gsd/
- [ ] 13 agents installed to .claude/agents/
- [ ] Shared directory copied to .claude/get-shit-done/
- [ ] Template variables all replaced (no {{VARIABLES}} remain)
- [ ] --help and --version flags work correctly
- [ ] Invalid flags show suggestions
- [ ] Re-installation overwrites files
- [ ] Success message includes next steps
- [ ] Source files (.github/, get-shit-done/) remain unchanged
- [ ] Progress messages show during installation
- [ ] Colors work in terminal (checkmarks visible)
  </how-to-verify>
  
  <resume-signal>
After verifying all items in the checklist pass, respond with one of:

**If all verification passes:**
"approved" or "✓ Phase 1 verified, all criteria met"

**If issues found:**
Describe the specific issue(s) encountered:
- Which verification step failed
- What was expected vs what happened
- Any error messages seen

Example: "Skills count is 28 instead of 29 - gsd-execute-plan missing"
  </resume-signal>
</task>

## Verification

After human verification checkpoint passes, final automated checks:

```bash
# 1. Verify all test files exist
ls -l tests/integration/install.test.js
ls -l tests/integration/template-rendering.test.js
ls -l tests/integration/error-scenarios.test.js
ls -l tests/helpers/test-env.js

# 2. Run full test suite one final time
npm test

# 3. Verify source files unchanged
git status --porcelain .github/ get-shit-done/ | wc -l
# Expected: 0 (no changes)

# 4. Verify package.json has test script
cat package.json | grep '"test":'

# 5. Generate coverage report (optional)
npm test -- --coverage
```

## Success Criteria

**Observable outcomes:**
- [ ] Integration tests verify complete installation flow
- [ ] Tests verify all 29 skills and 13 agents install correctly
- [ ] Tests verify template variables render correctly (no {{VARIABLES}} remain)
- [ ] Tests verify error scenarios (permissions, invalid flags)
- [ ] Tests run in isolated /tmp directories (TEST-01 verified)
- [ ] Human verification confirms installer works correctly
- [ ] All Phase 1 success criteria from ROADMAP.md are met

**Phase 1 Requirements Verified:**
- [x] INSTALL-01: NPX entry point works
- [x] INSTALL-02: File operations work correctly
- [x] INSTALL-03: Template rendering with variable substitution works
- [x] CLI-02: --claude flag works
- [x] CLI-05: --help and --version flags work
- [x] SAFETY-02: Path normalization works (~ expansion, path.join)
- [x] TEMPLATE-01: Templates copied from .github/ source
- [x] TEMPLATE-01B: Template conversion (variable injection) works
- [x] TEMPLATE-03: All template variables render correctly
- [x] TEST-01: All tests execute in /tmp (source files never modified)
- [x] TEST-02: Tests cleanup after success

**Required artifacts:**
- [x] tests/integration/install.test.js with complete flow tests
- [x] tests/integration/template-rendering.test.js with variable tests
- [x] tests/integration/error-scenarios.test.js with error handling tests
- [x] tests/helpers/test-env.js with isolated /tmp environment helper

**Wiring validated:**
- [x] Tests use test-env.js for isolation
- [x] Tests verify actual filesystem state
- [x] Tests verify variable substitution in real files
- [x] Tests verify error messages and formatting
- [x] All tests pass consistently

**Phase 1 Complete Deliverables:**
- [x] User can run `npx get-shit-done-multi --claude` and skills install
- [x] 29 skills install to ~/.claude/skills/gsd/
- [x] 13 agents install to ~/.claude/agents/
- [x] Shared directory copies to ~/.claude/get-shit-done/
- [x] Template variables replaced correctly
- [x] Installation completes in <30 seconds
- [x] --help and --version work correctly
- [x] Version displays as 2.0.0
- [x] Progress shows moderate detail with checkmarks
- [x] Success message includes next steps
- [x] Errors include actionable guidance
- [x] Source files never modified during testing

## Output

**Files created:**
- `tests/integration/install.test.js` — Complete installation flow tests
- `tests/integration/template-rendering.test.js` — Variable rendering tests
- `tests/integration/error-scenarios.test.js` — Error handling tests
- `tests/helpers/test-env.js` — Isolated test environment helper

**Files modified:**
- `package.json` — Added vitest dependencies and test scripts

**Phase 1 Status:** COMPLETE ✓

All success criteria from ROADMAP.md Phase 1 met:
1. ✓ User runs `npx get-shit-done-multi --claude` → installs to `~/.claude/skills/gsd/`
2. ✓ All 29 skills from `.github/skills/` installed
3. ✓ All 13 agents from `.github/agents/` installed
4. ✓ Shared directory copies to `.claude/get-shit-done/`
5. ✓ Template variables replaced correctly
6. ✓ Skill structure: `.claude/skills/gsd-<name>/SKILL.md`
7. ✓ Installation completes in <30 seconds
8. ✓ `--help` and `--version` show correct information
9. ✓ Version displays as 2.0.0

**Next phase:** Phase 2 - Multi-Platform Support (add Copilot and Codex adapters)
