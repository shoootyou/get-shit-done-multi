---
phase: 03-high-complexity-orchestrators
plan: 04
type: checkpoint
wave: 3
depends_on: ["03-02", "03-03"]
files_modified: []
autonomous: false
must_haves:
  goal: "Verify migrated orchestrators work end-to-end with subagent spawning and @-reference resolution"
  truths:
    - Execute-phase runs and spawns gsd-executor agents successfully
    - New-project runs and spawns 6 agents (4 researchers + synthesizer + roadmapper)
    - New-milestone runs and spawns 6 agents with milestone context
    - All @-references resolve correctly at runtime
    - Parallel spawning completes without errors
    - Structured returns parse correctly
    - Generated artifacts match legacy outputs
    - Commands discoverable in Claude interface
  artifacts:
    - None (verification only)
  wiring:
    - N/A (checkpoint plan)
  key_links:
    - Test execution validates specs ‚Üí generated commands ‚Üí runtime behavior
---

# Phase 3 Plan 04: E2E Orchestration Verification

**Objective:** Verify all three migrated orchestrators work end-to-end with subagent spawning, @-reference resolution, and structured returns

## Context

This is a verification checkpoint after migrating the three most complex orchestrators:
- **execute-phase** (03-02): Wave-based execution, dynamic spawning
- **new-project** (03-03): Parallel research, roadmap creation
- **new-milestone** (03-03): Milestone planning with existing project context

Unlike automated tests, this requires human verification of orchestration flows that involve user interaction, subagent spawning, and artifact creation.

**From Research (03-RESEARCH.md):**
- Critical: Parallel spawning must work (5x faster than sequential)
- Critical: @-references must resolve at runtime (templates, references, artifacts)
- Critical: Structured returns must parse correctly (## ROADMAP CREATED routing)

**What we're verifying:**
1. Commands install and are discoverable
2. Orchestration logic executes without errors
3. Subagents spawn in parallel as expected
4. @-references resolve correctly
5. Output quality matches legacy behavior

## Tasks

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Three high-complexity orchestrators migrated to spec format:
- gsd-execute-phase (wave-based execution)
- gsd-new-project (parallel research + roadmap)
- gsd-new-milestone (milestone planning)

Generated to .claude/ via template system, preserving:
- Parallel subagent spawning patterns
- @-reference context injection
- Multi-section XML structure
- Structured return parsing
  </what-built>
  
  <how-to-verify>
**Pre-flight checks:**

1. **Verify installation:**
```bash
# Check generated commands exist
ls -lh .claude/gsd-execute-phase.md
ls -lh .claude/gsd-new-project.md
ls -lh .claude/gsd-new-milestone.md

# Check line counts match expectations
wc -l .claude/gsd-execute-phase.md   # ~350 lines
wc -l .claude/gsd-new-project.md     # ~950 lines
wc -l .claude/gsd-new-milestone.md   # ~780 lines
```

2. **Verify structural integrity:**
```bash
# Check @-references preserved
grep -c "@" .claude/gsd-execute-phase.md   # ~2-3
grep -c "@" .claude/gsd-new-project.md     # ~4-5
grep -c "@" .claude/gsd-new-milestone.md   # ~4-5

# Check parallel spawning patterns
grep -c "Task(prompt=" .claude/gsd-execute-phase.md   # 1+ (dynamic)
grep -c "Task(prompt=" .claude/gsd-new-project.md     # 6+
grep -c "Task(prompt=" .claude/gsd-new-milestone.md   # 6+

# Check XML structure
grep -c "<.*>" .claude/gsd-execute-phase.md   # 10+
grep -c "<.*>" .claude/gsd-new-project.md     # 15+
grep -c "<.*>" .claude/gsd-new-milestone.md   # 15+
```

**Test 1: Execute-phase orchestration**

Prerequisites: Have a phase with 2-3 plans (use Phase 2 for testing)

```bash
# In Claude interface, run:
/gsd:execute-phase 2
```

**Expected behavior:**
1. Command discovers plans in .planning/phases/02-*
2. Groups plans by wave number
3. Spawns gsd-executor for each plan in wave (parallel)
4. Shows progress: "Spawning Wave 1: 02-01, 02-02..."
5. Waits for completion
6. Creates *-SUMMARY.md files
7. Updates STATE.md

**Verify:**
- [ ] Command runs without errors
- [ ] Spawns executors in parallel (see multiple agent indicators)
- [ ] @-references resolve (STATE.md loaded)
- [ ] Wave grouping works correctly
- [ ] SUMMARYs created after execution

**If checkpoint fails:** Spawn executor agents sequentially, missing @-references, or wave logic broken


**Test 2: New-project orchestration**

Prerequisites: Clean test directory or new codebase

```bash
# In Claude interface, run:
/gsd:new-project
```

**Expected behavior:**
1. Asks for project details (name, domain, description)
2. Creates .planning/ directory structure
3. Shows: "‚óÜ Spawning 4 researchers in parallel..."
4. Spawns 4 gsd-project-researcher agents simultaneously
5. Each researcher writes to .planning/research/*.md
6. Spawns gsd-research-synthesizer
7. Spawns gsd-roadmapper
8. Presents roadmap for approval
9. Commits on approval

**Verify:**
- [ ] Command runs without errors
- [ ] 4 researchers spawn in parallel (not sequential)
- [ ] @-references resolve (questioning.md, ui-brand.md, templates)
- [ ] Research files created (STACK.md, FEATURES.md, etc.)
- [ ] SUMMARY.md synthesized
- [ ] ROADMAP.md created with phases
- [ ] Structured return parsing works (approval flow)

**Performance check:**
- Research phase should take ~2-3 minutes (parallel)
- If 5-10 minutes, spawning became sequential (FAIL)

**If checkpoint fails:** Sequential spawning, missing @-references, no roadmap created, errors during research


**Test 3: New-milestone orchestration**

Prerequisites: Existing project with PROJECT.md

```bash
# In Claude interface, run:
/gsd:new-milestone
```

**Expected behavior:**
1. Validates existing PROJECT.md exists
2. Asks for milestone details
3. Determines milestone number
4. Creates milestones/XX-name/ directory
5. Spawns 4 researchers with milestone_context: subsequent
6. Spawns synthesizer
7. Spawns roadmapper
8. Creates milestone ROADMAP.md

**Verify:**
- [ ] Command runs without errors
- [ ] Validates existing project first
- [ ] 4 researchers spawn in parallel
- [ ] milestone_context differs from new-project (subsequent vs greenfield)
- [ ] Milestone directory created
- [ ] Milestone ROADMAP.md created

**If checkpoint fails:** Doesn't validate project, sequential spawning, wrong context


**Test 4: @-Reference resolution**

Check that @-references work at runtime:

```bash
# Verify reference files exist
test -f ~/.claude/get-shit-done/references/questioning.md
test -f ~/.claude/get-shit-done/references/ui-brand.md
test -f ~/.claude/get-shit-done/templates/project.md
test -f ~/.claude/get-shit-done/templates/requirements.md

# If any missing, that's why orchestrators would fail
```

During orchestrator runs, watch for errors like:
- "File not found: ~/.claude/get-shit-done/..."
- "Unable to load @-reference..."

**Verify:**
- [ ] No missing reference errors during execution
- [ ] Templates load correctly
- [ ] Planning artifacts (@.planning/*) accessible


**Test 5: Structured return handling**

During new-project or new-milestone run:

After roadmapper spawns, it should return one of:
```markdown
## ROADMAP CREATED
**Phase:** 1
**Summary:** [details]
```

OR

```markdown
## ROADMAP BLOCKED
**Blocked by:** [issue]
**Options:** [list]
```

**Verify:**
- [ ] Orchestrator correctly routes based on return type
- [ ] CREATED ‚Üí shows roadmap, asks approval
- [ ] BLOCKED ‚Üí presents issue, offers resolution
- [ ] Markdown header parsing works (not regex fragility)


**Test 6: Command discoverability**

In Claude interface:

```bash
# Type /gsd: and check autocomplete
# Should see:
/gsd:execute-phase
/gsd:new-project
/gsd:new-milestone
```

**Verify:**
- [ ] All three commands appear in autocomplete
- [ ] Descriptions are accurate
- [ ] Commands invoke successfully when selected


**Summary verification:**

After running all tests:
- [ ] All three orchestrators run without errors
- [ ] Parallel spawning works (performance validated)
- [ ] @-references resolve correctly
- [ ] Structured returns parse correctly
- [ ] Output quality matches legacy behavior
- [ ] No regressions vs legacy commands
  </how-to-verify>
  
  <resume-signal>
**If all tests pass:**
Type: `approved` to proceed to Phase 4 planning

**If issues found:**
Document specific failures:
```
Issue: [description]
Command: [which orchestrator]
Symptom: [error message or behavior]
Impact: [does it block Phase 4?]
```

Then either:
1. Create gap closure plan if fixable
2. Discuss if architectural issue
  </resume-signal>
</task>

## Verification

Human verification required. Automated checks insufficient for:
- Multi-agent spawning dynamics
- @-reference runtime resolution
- User interaction flows
- Approval/revision handling
- Performance characteristics (parallel vs sequential)

## Success Criteria

- [x] All three orchestrators run end-to-end without errors
- [x] Parallel spawning confirmed (4-6 agents simultaneously)
- [x] @-references resolve at runtime (no missing file errors)
- [x] Structured returns parse and route correctly
- [x] Artifacts created match legacy behavior
- [x] Commands discoverable in Claude interface
- [x] Performance acceptable (parallel not sequential)
- [x] No regressions vs legacy commands

## Output

**Deliverable:**
Human verification report:
- ‚úÖ Passed tests (list)
- ‚ö†Ô∏è Issues found (if any, with details)
- üéØ Next steps: Proceed to Phase 4 or create gap closure plans

**What this unlocks:**
- Phase 4: Mid-complexity commands (plan-phase, research-phase, debug, map-codebase)
- Confidence that migration pattern works for orchestrators
- Validation that template system handles complex commands
- Proof that @-references and parallel spawning survive migration
