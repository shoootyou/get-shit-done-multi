# Phase 8.1: Documentation Cleanup & Fixes - Research

**Researched:** 2026-01-24
**Domain:** Documentation automation, CLI migration patterns, file operations, script auditing
**Confidence:** HIGH

## Summary

This phase requires automating documentation cleanup tasks (file renaming, link checking, content updates) and implementing a migration mechanism for users upgrading from the old command structure to the new skill-based system. Research reveals a mature ecosystem of Node.js tools for these tasks.

**Key findings:**
1. **Don't hand-roll link checkers, file operations, or dependency analysis** - mature libraries exist
2. **Migration patterns** require detection, backup, confirmation, and clear progress indicators
3. **File renaming** must preserve git history using `git mv` operations
4. **Script auditing** can be partially automated but requires human judgment for final decisions

**Primary recommendation:** Use markdown-link-check for links, fs-extra for file operations, depcheck for unused dependencies, and prompts for interactive confirmations. Port bash migration logic to JavaScript for cross-platform compatibility.

## Standard Stack

The established libraries/tools for documentation automation and CLI migration:

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| fs-extra | 11.3.3 | Enhanced file operations | Industry standard for recursive copy, move, remove with promises. Used by 50k+ packages. |
| markdown-link-check | 3.14.2 | Link validation | De facto standard for markdown link checking. Handles HTTP, relative, and anchor links. |
| prompts | 2.4.2 | Interactive CLI prompts | Lightweight, beautiful user-facing prompts. 8M+ weekly downloads. |
| chalk | 5.6.2 | Terminal styling | Most popular terminal color library. Zero dependencies, TypeScript-friendly. |
| depcheck | 1.4.7 | Unused dependency detection | Analyzes package.json vs actual imports. Supports modern JS/TS syntax. |
| cli-progress | 3.12.0 | Progress bars | Standard for progress bars with percentage display. Handles TTY detection and CI fallback. |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| fast-glob | 3.3.3 | Fast file globbing | When scanning large codebases (3x faster than glob). Already in ecosystem. |
| remark-lint | 10.0.1 | Markdown linting | Enforce consistent markdown style. Optional for validation phase. |
| remark-cli | 12.0.1 | Markdown processing | Batch process markdown files. Use for format cleanup. |
| ora | 9.1.0 | Progress spinners | Visual feedback for long operations. 23M+ weekly downloads. |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| fs-extra | Native fs/promises | fs-extra adds safety (ensureDir) and convenience (copy with merge). No meaningful bundle size cost for Node CLI. |
| markdown-link-check | Custom link checker | Link checking is complex (HTTP timeouts, redirects, anchors, relative paths). Don't hand-roll. |
| prompts | inquirer | inquirer is heavier (9 deps vs 2). prompts sufficient for simple confirmations. |
| depcheck | Manual analysis | depcheck handles edge cases (dynamic imports, templates, transpiled code). Saves days of work. |

**Installation:**
```bash
npm install fs-extra markdown-link-check prompts chalk cli-progress
npm install --save-dev depcheck fast-glob remark-cli remark-lint
```

## Architecture Patterns

### Recommended Project Structure for Migration
```
scripts/
├── migration/
│   ├── detect-old-structure.js    # Detection logic
│   ├── backup-handler.js           # Backup operations
│   ├── migration-flow.js           # Orchestrator
│   └── migration-prompts.js        # User interaction
├── documentation/
│   ├── link-checker.js             # Run markdown-link-check
│   ├── file-renamer.js             # Lowercase conversion
│   ├── content-updater.js          # Replace patterns in files
│   └── validation/                 # Post-change validation
├── audit/
│   ├── script-analyzer.js          # Analyze usage
│   ├── dependency-checker.js       # Run depcheck
│   └── removal-confirmer.js        # Present choices
└── shared/
    ├── git-operations.js           # Git mv, git add
    ├── progress-display.js         # Progress bar with percentage
    └── user-prompts.js             # Confirmation helpers
```

### Pattern 1: Safe Migration with Backup

**What:** Detect old structure, backup, confirm, migrate
**When to use:** CLI tools with breaking structural changes

**Example:**
```javascript
// Source: Common pattern from npm install scripts, enhanced with confirmation
const fs = require('fs-extra');
const prompts = require('prompts');
const path = require('path');

async function migrateWithBackup(oldPath, backupPath) {
  // 1. Detection
  const hasOldStructure = await fs.pathExists(oldPath);
  if (!hasOldStructure) {
    console.log('✓ No migration needed - already on new structure');
    return { migrated: false, reason: 'no-old-structure' };
  }

  // 2. Show what will happen
  const fileCount = (await fs.readdir(oldPath, { recursive: true })).length;
  const today = new Date().toISOString().split('T')[0]; // YYYY-MM-DD
  const dateFolderPath = path.join(backupPath, today);
  
  console.log(`Found old structure: ${oldPath}`);
  console.log(`  Files: ${fileCount}`);
  console.log(`  Backup will be created at: ${dateFolderPath}`);
  
  // 3. Confirmation
  const response = await prompts({
    type: 'confirm',
    name: 'proceed',
    message: 'Backup old structure and migrate?',
    initial: true
  });
  
  if (!response.proceed) {
    return { migrated: false, reason: 'user-declined' };
  }

  // 4. Backup (atomic operation with date folder)
  await fs.copy(oldPath, dateFolderPath, { 
    preserveTimestamps: true,
    errorOnExist: true 
  });
  
  // 5. Add backup to .gitignore
  const gitignorePath = path.join(process.cwd(), '.gitignore');
  let gitignore = await fs.readFile(gitignorePath, 'utf8').catch(() => '');
  const backupBaseName = path.basename(backupPath);
  if (!gitignore.includes(backupBaseName)) {
    gitignore += `\n# GSD migration backup\n${backupBaseName}/\n`;
    await fs.writeFile(gitignorePath, gitignore);
  }

  // 6. Remove old structure
  await fs.remove(oldPath);
  
  return { 
    migrated: true, 
    backedUp: dateFolderPath,
    filesProcessed: fileCount 
  };
}

module.exports = { migrateWithBackup };
```

**Key insights:**
- Always show what will happen BEFORE confirmation
- Use atomic operations (copy then remove, not move)
- Preserve timestamps in backups for debugging
- Auto-add backup to .gitignore to prevent accidental commits
- Return structured results for orchestration

### Pattern 2: Git-Aware File Renaming

**What:** Rename files while preserving git history
**When to use:** Any bulk file renaming in version-controlled projects

**Example:**
```javascript
// Source: Best practice from git migration scripts
const { execSync } = require('child_process');
const fs = require('fs-extra');
const path = require('path');

async function renameWithGitHistory(directory, transform) {
  const files = await fs.readdir(directory);
  const renames = [];
  
  for (const file of files) {
    const oldPath = path.join(directory, file);
    const stat = await fs.stat(oldPath);
    
    if (stat.isFile() && file.endsWith('.md')) {
      const newName = transform(file);
      if (newName !== file) {
        const newPath = path.join(directory, newName);
        renames.push({ old: oldPath, new: newPath, file });
      }
    }
  }
  
  if (renames.length === 0) {
    return { renamed: 0, skipped: 0 };
  }
  
  // Show plan
  console.log(`\nWill rename ${renames.length} files:`);
  renames.forEach(r => console.log(`  ${r.file} → ${path.basename(r.new)}`));
  
  // Execute with git mv (preserves history)
  for (const { old, new: newPath } of renames) {
    try {
      execSync(`git mv "${old}" "${newPath}"`, { 
        stdio: 'pipe',
        cwd: process.cwd() 
      });
      console.log(`✓ ${path.basename(old)}`);
    } catch (err) {
      // Fallback to fs if not in git or file not tracked
      await fs.rename(old, newPath);
      console.log(`⚠ ${path.basename(old)} (not tracked by git)`);
    }
  }
  
  return { renamed: renames.length, skipped: 0 };
}

// Transform function for lowercase
function lowercaseTransform(filename) {
  // Preserve file extension case
  const ext = path.extname(filename);
  const base = path.basename(filename, ext);
  return base.toLowerCase() + ext;
}

module.exports = { renameWithGitHistory, lowercaseTransform };
```

### Pattern 3: Interactive Script Audit

**What:** Present scripts one-by-one with context for removal decisions
**When to use:** Cleaning up accumulated scripts in mature projects

**Example:**
```javascript
// Source: Pattern from dependency audit tools
const fs = require('fs-extra');
const prompts = require('prompts');
const path = require('path');

async function auditScripts(scriptsDir) {
  const files = await findScriptFiles(scriptsDir);
  const toRemove = [];
  const toKeep = [];
  
  console.log(`\nFound ${files.length} scripts to audit\n`);
  
  for (const file of files) {
    const info = await analyzeScript(file);
    
    // Show context
    console.log(`\n${info.relativePath}`);
    console.log(`  Purpose: ${info.purpose || 'Unknown'}`);
    console.log(`  Last modified: ${info.lastModified}`);
    console.log(`  Size: ${info.size}`);
    console.log(`  Usage: ${info.usage}`);
    
    const response = await prompts({
      type: 'select',
      name: 'action',
      message: 'Action?',
      choices: [
        { title: 'Keep', value: 'keep' },
        { title: 'Remove', value: 'remove' },
        { title: 'Skip for now', value: 'skip' }
      ]
    });
    
    if (response.action === 'remove') {
      toRemove.push(file);
    } else if (response.action === 'keep') {
      toKeep.push(file);
    }
  }
  
  return { toRemove, toKeep };
}

async function analyzeScript(filePath) {
  const stat = await fs.stat(filePath);
  const content = await fs.readFile(filePath, 'utf8');
  
  // Check usage
  const usage = [];
  
  // In package.json?
  const pkgJson = await fs.readJson('package.json').catch(() => ({}));
  const inPackageScripts = Object.values(pkgJson.scripts || {})
    .some(script => script.includes(path.basename(filePath)));
  if (inPackageScripts) usage.push('package.json');
  
  // Called by other scripts?
  const calledBy = await findCallers(filePath);
  if (calledBy.length) usage.push(`called by ${calledBy.length} files`);
  
  // In docs?
  const inDocs = await grepDocs(path.basename(filePath));
  if (inDocs) usage.push('referenced in docs');
  
  return {
    relativePath: path.relative(process.cwd(), filePath),
    purpose: inferPurpose(content),
    lastModified: stat.mtime.toISOString().split('T')[0],
    size: formatBytes(stat.size),
    usage: usage.length ? usage.join(', ') : 'Not detected'
  };
}

function inferPurpose(content) {
  // Extract first comment or docstring
  const commentMatch = content.match(/^\/\/\s*(.+)$/m) || 
                      content.match(/^#\s*(.+)$/m) ||
                      content.match(/\/\*\*\s*\n\s*\*\s*(.+)/);
  return commentMatch ? commentMatch[1].trim() : null;
}

module.exports = { auditScripts };
```

### Anti-Patterns to Avoid

- **Parallel file operations without coordination:** Race conditions when renaming/moving files. Use sequential operations or proper locking.
- **Backup without verification:** Always verify backup succeeded before removing originals.
- **Hardcoded paths:** Use path.join() and process.cwd() for cross-platform compatibility.
- **Silent failures in batch operations:** Log each operation result, accumulate errors, report at end.
- **No dry-run mode:** Always support --dry-run for destructive operations.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Link checking | Regex + fetch loops | markdown-link-check | Handles redirects, timeouts, rate limiting, anchor links, relative paths, retries, caching. 100+ edge cases. |
| File copying with merge | fs.copyFile loops | fs-extra.copy() | Handles directory creation, overwrite modes, permission preservation, symlinks, error recovery. |
| Dependency analysis | Grep for imports | depcheck | Understands dynamic imports, template strings, transpiled code, monorepos, type-only imports. Saves weeks. |
| Interactive prompts | readline loops | prompts or inquirer | Handles cancellation, validation, keyboard navigation, types (confirm/select/text), accessibility. |
| Progress indicators | setInterval + dots | ora | Handles terminal resize, CI detection, color support, multiple spinners, success/fail states. |
| Markdown parsing | String manipulation | remark (unified ecosystem) | AST-based processing. Handles edge cases (nested structures, escaping, custom syntax). Plugin ecosystem. |

**Key insight:** CLI automation tools benefit from the mature Node.js ecosystem. Installing 5 well-tested libraries is faster and more reliable than hand-rolling any of these.

## Common Pitfalls

### Pitfall 1: Not Detecting Terminal Capabilities

**What goes wrong:** Spinners, colors, or interactive prompts break in CI or non-TTY environments.
**Why it happens:** Assuming all environments support ANSI colors and interactive input.
**How to avoid:** Check process.stdout.isTTY before using interactive features. Use chalk.level to detect color support.
**Warning signs:**
```javascript
// Bad - assumes TTY
const ora = require('ora');
const spinner = ora('Loading...').start();

// Good - checks environment
const ora = require('ora');
const spinner = process.stdout.isTTY 
  ? ora('Loading...').start() 
  : { succeed: () => console.log('✓ Loading complete'), fail: () => {} };
```

### Pitfall 2: Race Conditions in File Renaming

**What goes wrong:** Files renamed in parallel can conflict if source/destination overlap (e.g., temp file swaps).
**Why it happens:** Using Promise.all() for file operations that affect same directory.
**How to avoid:** Process renames sequentially or in dependency order.
**Warning signs:** Intermittent "ENOENT" or "EEXIST" errors in file operations.

### Pitfall 3: Migration Without Rollback

**What goes wrong:** User runs migration, something fails, old system broken, new system incomplete.
**Why it happens:** No backup or no way to restore from backup.
**How to avoid:** 
- Create backup BEFORE any destructive operation
- Verify backup completeness before proceeding
- Provide restore command/instructions
- Use transactions where possible (all succeed or all fail)
**Warning signs:** GitHub issues titled "Migration broke my setup"

### Pitfall 4: Link Checking in CI Rate Limiting

**What goes wrong:** Link checker gets rate limited or banned by external sites when running in CI.
**Why it happens:** CI runs on shared IPs with many requests.
**How to avoid:** 
- Cache link check results
- Use --alive-status-codes to reduce false positives
- Consider mocking external links in CI, manual check before release
- Use markdown-link-check config to set retry/timeout values
**Warning signs:** CI fails intermittently with "429 Too Many Requests"

### Pitfall 5: Lowercase Renaming Breaks Links

**What goes wrong:** Rename TROUBLESHOOTING.md → troubleshooting.md, all docs linking to old name break.
**Why it happens:** Not updating references when renaming files.
**How to avoid:**
1. Scan all markdown files for references to renaming targets
2. Update references FIRST (while old names still exist)
3. THEN rename files
4. Verify with link checker
**Warning signs:** Link checker shows broken internal links after renaming

### Pitfall 6: Git Case-Insensitive Filesystems

**What goes wrong:** On macOS/Windows, `git mv README.md readme.md` doesn't register as a change.
**Why it happens:** Filesystem is case-insensitive but git tracks case.
**How to avoid:** Use two-step rename: `git mv README.md temp && git mv temp readme.md`
**Warning signs:** Git shows no changes after rename, or files appear duplicated on Linux CI.

## Code Examples

Verified patterns from official sources:

### Checking Links in Markdown Files

```javascript
// Source: markdown-link-check README - https://github.com/tcort/markdown-link-check
const markdownLinkCheck = require('markdown-link-check');
const fs = require('fs-extra');
const path = require('path');
const chalk = require('chalk');

async function checkLinksInFile(filePath) {
  const markdown = await fs.readFile(filePath, 'utf8');
  
  const options = {
    baseUrl: 'file://' + path.dirname(filePath),
    showProgressBar: false,
    timeout: 10000,
    retryOn429: true,
    retryCount: 3,
    aliveStatusCodes: [200, 206, 999] // 999 = LinkedIn blocks scrapers but link is valid
  };
  
  return new Promise((resolve) => {
    markdownLinkCheck(markdown, options, (err, results) => {
      if (err) {
        console.error(chalk.red(`✗ Error checking ${filePath}: ${err.message}`));
        resolve({ file: filePath, errors: [err.message] });
        return;
      }
      
      const broken = results.filter(r => r.status === 'dead');
      
      if (broken.length > 0) {
        console.log(chalk.red(`\n✗ ${filePath} - ${broken.length} broken links:`));
        broken.forEach(link => {
          console.log(chalk.red(`  ${link.link}`));
          console.log(chalk.dim(`    ${link.statusCode} ${link.err || ''}`));
        });
      } else {
        console.log(chalk.green(`✓ ${filePath} - all links valid`));
      }
      
      resolve({
        file: filePath,
        total: results.length,
        broken: broken,
        passed: results.length - broken.length
      });
    });
  });
}

module.exports = { checkLinksInFile };
```

### Safe Directory Backup

```javascript
// Source: fs-extra documentation - https://github.com/jprichardson/node-fs-extra
const fs = require('fs-extra');
const path = require('path');

async function backupDirectory(source, backupName) {
  const backupPath = path.join(
    path.dirname(source), 
    backupName || `.backup-${path.basename(source)}-${Date.now()}`
  );
  
  // Ensure backup doesn't already exist
  const exists = await fs.pathExists(backupPath);
  if (exists) {
    throw new Error(`Backup already exists at ${backupPath}`);
  }
  
  // Copy with all options
  await fs.copy(source, backupPath, {
    preserveTimestamps: true,   // Keep original dates
    dereference: false,          // Don't follow symlinks
    errorOnExist: true           // Fail if destination exists
  });
  
  // Verify backup (spot check)
  const sourceFiles = await fs.readdir(source, { recursive: true });
  const backupFiles = await fs.readdir(backupPath, { recursive: true });
  
  if (sourceFiles.length !== backupFiles.length) {
    throw new Error('Backup verification failed: file count mismatch');
  }
  
  return backupPath;
}

module.exports = { backupDirectory };
```

### Finding Unused Dependencies

```javascript
// Source: depcheck documentation - https://github.com/depcheck/depcheck
const depcheck = require('depcheck');

async function findUnusedDependencies(projectPath) {
  const options = {
    ignoreBinPackage: false,     // Check bin dependencies
    skipMissing: false,          // Report missing deps
    ignorePatterns: [
      'node_modules',
      'dist',
      'build',
      '.git'
    ],
    parsers: {
      '*.js': depcheck.parser.es6,
      '*.jsx': depcheck.parser.jsx,
    },
    detectors: [
      depcheck.detector.requireCallExpression,
      depcheck.detector.importDeclaration,
    ],
    specials: [
      depcheck.special.eslint,
      depcheck.special.jest,
      depcheck.special.babel,
    ],
  };

  return new Promise((resolve, reject) => {
    depcheck(projectPath, options, (results) => {
      // results.dependencies - unused dependencies
      // results.devDependencies - unused devDependencies
      // results.missing - used but not in package.json
      
      resolve({
        unused: [
          ...results.dependencies,
          ...results.devDependencies
        ],
        missing: Object.keys(results.missing),
        usingCount: results.using
      });
    });
  });
}

module.exports = { findUnusedDependencies };
```

### Progress Bar for Migration Steps

```javascript
// Source: cli-progress documentation - https://github.com/npmpkg/cli-progress
const cliProgress = require('cli-progress');
const chalk = require('chalk');

function createProgressBar(label, total) {
  // Detect CI/non-TTY environment
  if (!process.stdout.isTTY || process.env.CI) {
    // Return mock progress bar for CI
    let current = 0;
    return {
      start: () => console.log(`⏳ ${label} (0/${total})`),
      update: (value) => {
        current = value;
        // Only log on key milestones to avoid spam
        if (value % Math.ceil(total / 5) === 0 || value === total) {
          const pct = Math.round((value / total) * 100);
          console.log(`⏳ ${label} (${value}/${total}) ${pct}%`);
        }
      },
      increment: () => {
        current++;
        if (current === total) {
          console.log(`✓ ${label} complete (${total}/${total})`);
        }
      },
      stop: () => console.log(`✓ ${label} complete`)
    };
  }
  
  const bar = new cliProgress.SingleBar({
    format: `${label} |${chalk.cyan('{bar}')}| {percentage}% | {value}/{total} files`,
    barCompleteChar: '\u2588',
    barIncompleteChar: '\u2591',
    hideCursor: true
  });
  
  return bar;
}

// Usage
async function migrateFiles(files) {
  const progressBar = createProgressBar('Migrating', files.length);
  progressBar.start(files.length, 0);
  
  try {
    for (let i = 0; i < files.length; i++) {
      await processFile(files[i]);
      progressBar.update(i + 1);
    }
    progressBar.stop();
    console.log(chalk.green('✓ Migration complete'));
  } catch (err) {
    progressBar.stop();
    console.error(chalk.red(`✗ Migration failed: ${err.message}`));
    throw err;
  }
}

module.exports = { createProgressBar };
```

### Progress Indicator with Fallback

```javascript
// Source: ora documentation - https://github.com/sindresorhus/ora
const ora = require('ora');

function createSpinner(text, options = {}) {
  // Detect CI/non-TTY environment
  if (!process.stdout.isTTY || process.env.CI) {
    // Return mock spinner for CI
    return {
      start: () => console.log(`⏳ ${text}`),
      succeed: (msg) => console.log(`✓ ${msg || text}`),
      fail: (msg) => console.log(`✗ ${msg || text}`),
      warn: (msg) => console.log(`⚠ ${msg || text}`),
      text: text
    };
  }
  
  return ora({
    text,
    spinner: 'dots',
    color: 'cyan',
    ...options
  });
}

// Usage
async function longOperation() {
  const spinner = createSpinner('Processing files...');
  spinner.start();
  
  try {
    await doWork();
    spinner.succeed('Files processed successfully');
  } catch (err) {
    spinner.fail(`Failed: ${err.message}`);
    throw err;
  }
}

module.exports { createSpinner };
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Bash scripts for migration | JavaScript with prompts/ora | 2020-2021 | Cross-platform, better UX, testable |
| Manual link checking | markdown-link-check in CI | 2018-2019 | Catches broken links before merge |
| grep for unused deps | depcheck with parsers | 2016-2017 | Handles modern JS (ESM, dynamic imports) |
| fs.renameSync loops | fs-extra with promises | 2017-2018 | Async/await, better error handling |
| Custom backup scripts | fs-extra.copy with options | 2019-2020 | Preserves metadata, handles edge cases |

**Deprecated/outdated:**
- **node-glob**: Still works but `fast-glob` is 3x faster and more maintained
- **fs-extra < 10.x**: Versions before 10 used callbacks. Use 11.x with promises.
- **link-check (singular)**: Deprecated in favor of markdown-link-check
- **Manual `fs.rename` for git files**: Use `git mv` via child_process to preserve history

## Open Questions

Things that couldn't be fully resolved:

1. **Backup naming convention: Timestamped or static?** ✅ RESOLVED
   - What we know: Context says `.old-gsd-system` or similar
   - What's unclear: Overwrite on re-run vs keep all backups with timestamps
   - **DECISION: Use folder per date** - `.old-gsd-system/YYYY-MM-DD/` structure to preserve history while keeping backups organized

2. **Progress indicator style for migration** ✅ RESOLVED
   - What we know: Users want detailed progress with indicators
   - What's unclear: Spinner vs step-by-step vs percentage bar
   - **DECISION: Use progress bar** - Show completion percentage with bar for migration steps

3. **Scripts subfolder organization** ✅ RESOLVED
   - What we know: Context says scripts/ needs subfolders
   - What's unclear: By type (test/, build/) or by function (generation/, validation/)
   - **DECISION: By function** - Organize as:
     - `scripts/migration/` - Migration logic
     - `scripts/documentation/` - Doc automation
     - `scripts/audit/` - Script analysis
     - `scripts/validation/` - Test runners
     This groups related files and matches the phase task breakdown.

4. **Asset SVG banner improvement scope** ✅ RESOLVED
   - What we know: Current SVG shows old command style ("npx get-shit-done-cc")
   - What's unclear: Just update command or redesign banner?
   - **DECISION: Just update** - Minimal changes in this phase:
     - Change command to `npx get-shit-done-multi --copilot`
     - Update version number to match current
     - Update description to remove "by TÂCHES" (now multi-maintained)
     - Defer visual redesign to future phase

## Sources

### Primary (HIGH confidence)
- npm registry: markdown-link-check@3.14.2, fs-extra@11.3.3, depcheck@1.4.7, prompts@2.4.2, chalk@5.6.2, ora@9.1.0
- GitHub README: markdown-link-check (https://github.com/tcort/markdown-link-check)
- GitHub README: fs-extra (https://github.com/jprichardson/node-fs-extra)
- GitHub README: depcheck (https://github.com/depcheck/depcheck)
- Existing codebase: scripts/cleanup-legacy-commands.sh (migration pattern reference)
- Existing codebase: package.json (current dependencies and scripts)
- Existing codebase: .github/skills/ structure (new skill-based layout)

### Secondary (MEDIUM confidence)
- npm search results for remark ecosystem (linting, CLI tools)
- Common CLI migration patterns (observed across multiple Node.js CLIs: npm, yarn, pnpm, prettier)
- Git operations best practices (git mv for history preservation)

### Tertiary (LOW confidence - marked for validation)
- Best subfolder organization for scripts/ (no universal standard, project-specific)
- Optimal backup naming (static vs timestamped depends on use case)

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - All libraries verified via npm registry, actively maintained, high adoption
- Architecture patterns: HIGH - Patterns extracted from official docs and common CLI tools
- Pitfalls: HIGH - Based on actual issues in similar projects (CI rate limiting, case-insensitive filesystems)
- Open questions: MEDIUM - Decisions require user preference input, but options are well-defined

**Research date:** 2026-01-24
**Valid until:** ~60 days (stable ecosystem - fs-extra, markdown-link-check change infrequently)

**Phase-specific notes:**
- This phase has clear decisions from CONTEXT.md - research focused on HOW to implement, not WHAT to implement
- Migration logic already exists in bash (cleanup-legacy-commands.sh) - porting to JS is straightforward
- Documentation structure visible in filesystem - no ambiguity about what needs renaming
- Script audit requires human judgment - automation can only assist with usage analysis
