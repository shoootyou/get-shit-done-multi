---
phase: 03-command-system
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - bin/lib/command-system/recorder.js
  - bin/lib/command-system/verifier.js
  - bin/test-command-system.js
autonomous: false

must_haves:
  truths:
    - "Command executions can be recorded with timestamp, CLI, command, args, and result"
    - "User can compare command results across different CLIs"
    - "Command system can be tested automatically"
    - "Installation verification confirms all commands are accessible"
  artifacts:
    - path: "bin/lib/command-system/recorder.js"
      provides: "Command execution recording to JSON"
      exports: ["recordExecution", "loadRecordings", "compareExecutions"]
      min_lines: 80
    - path: "bin/lib/command-system/verifier.js"
      provides: "Command system verification and validation"
      exports: ["verifyCommands", "verifyCommandAccessibility"]
      min_lines: 60
    - path: "bin/test-command-system.js"
      provides: "Automated test suite for command system"
      min_lines: 100
  key_links:
    - from: "bin/lib/command-system/recorder.js"
      to: ".planning/command-recordings/"
      via: "JSON file writes"
      pattern: "writeFile.*\\.json"
    - from: "bin/lib/command-system/verifier.js"
      to: "bin/lib/command-system/registry.js"
      via: "registry.list() verification"
      pattern: "registry\\.list\\("
    - from: "bin/test-command-system.js"
      to: "bin/lib/command-system/loader.js"
      via: "loadCommands() test"
      pattern: "loadCommands\\("
---

<objective>
Implement command recording for cross-CLI comparison and create verification tools for installation testing.

Purpose: Enable users to record command executions for documentation (CMD-06 requirement), compare results across CLIs (CMD-05 requirement), and verify post-installation that all commands are accessible (INSTALL-08 requirement).

Output: Recording system for command executions, verification tools, and automated test suite.
</objective>

<execution_context>
@.github/skills/get-shit-done/get-shit-done/workflows/execute-plan.md
@.github/skills/get-shit-done/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@.planning/phases/03-command-system/03-RESEARCH.md
@bin/lib/command-system/registry.js
@bin/lib/command-system/loader.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create command execution recorder</name>
  <files>bin/lib/command-system/recorder.js</files>
  <action>
    Create `bin/lib/command-system/recorder.js` to record command executions for CMD-05 and CMD-06 requirements.
    
    Components:
    1. recordExecution(commandName, args, result, cli, duration)
       - Create recording object: { timestamp: Date.now(), cli, command: commandName, args, result, duration, success: result.success }
       - Generate filename: `.planning/command-recordings/${cli}-${commandName}-${timestamp}.json`
       - Write to file using fs/promises.writeFile()
       - Create directory if it doesn't exist (mkdir with recursive: true)
       - Return recording object
    
    2. loadRecordings(commandName = null)
       - Read `.planning/command-recordings/*.json` files
       - Parse each JSON file
       - Filter by commandName if provided
       - Sort by timestamp descending (newest first)
       - Return array of recordings
    
    3. compareExecutions(commandName)
       - Load all recordings for commandName
       - Group by CLI
       - Compare results (success rate, duration, outputs)
       - Return comparison report: { command, byCLI: { 'claude-code': [...], 'copilot-cli': [...] }, summary: {...} }
    
    Use fs/promises for all file operations. Import path.join for cross-platform paths. Add JSDoc documentation.
    
    Research mentions JSON logs as recommendation - follow this approach.
  </action>
  <verify>
    ```bash
    # Test recording
    node -e "
    import('./bin/lib/command-system/recorder.js').then(m => {
      return m.recordExecution('test-command', ['arg1'], { success: true }, 'claude-code', 123);
    }).then(recording => {
      console.log('Recorded:', recording.command);
      console.log('File created:', recording.timestamp);
    });
    "
    
    # Verify file created
    ls .planning/command-recordings/*.json | head -1
    ```
    Should output: Recording confirmation and JSON file existence
  </verify>
  <done>
    - recorder.js exists with recordExecution()
    - Records executions to .planning/command-recordings/
    - JSON format includes timestamp, CLI, command, args, result, duration
    - loadRecordings() retrieves past executions
    - compareExecutions() groups recordings by CLI
    - Creates directory structure automatically
    - All functions have JSDoc documentation
  </done>
</task>

<task type="auto">
  <name>Task 2: Create command system verifier</name>
  <files>bin/lib/command-system/verifier.js</files>
  <action>
    Create `bin/lib/command-system/verifier.js` for INSTALL-08 requirement (post-install verification).
    
    Components:
    1. verifyCommands()
       - Load commands using loader.loadCommands()
       - Get all commands from registry.list()
       - Check each command has required metadata fields: name, description
       - Verify command count matches expected (24 commands)
       - Return { success: boolean, commandCount: number, issues: [] }
    
    2. verifyCommandAccessibility(cli)
       - Import detectCLI from '../cli-detector.js'
       - Verify current CLI matches expected
       - Check command files exist in appropriate directory:
         - Claude: ~/.claude/get-shit-done/commands/gsd/
         - Copilot: .github/skills/get-shit-done/commands/gsd/
         - Codex: .codex/skills/get-shit-done/commands/gsd/
       - Use Phase 1's getConfigPaths() for path resolution
       - Return { accessible: boolean, cli, path: string, fileCount: number }
    
    3. Import registry from './registry.js'
    4. Import loadCommands from './loader.js'
    5. Import getConfigPaths from '../path-utils.js' (Phase 1)
    
    Add JSDoc. Reference Phase 1's path-utils.js for consistent path handling.
  </action>
  <verify>
    ```bash
    # Test verification
    node -e "
    import('./bin/lib/command-system/verifier.js').then(m => {
      return m.verifyCommands();
    }).then(result => {
      console.log('Verification:', result.success ? 'PASS' : 'FAIL');
      console.log('Commands found:', result.commandCount);
      if (result.issues.length > 0) {
        console.log('Issues:', result.issues);
      }
    });
    "
    ```
    Should output: Verification PASS with 24 commands
  </verify>
  <done>
    - verifier.js exists with verifyCommands()
    - Checks all 24 commands loaded correctly
    - Validates required metadata fields present
    - verifyCommandAccessibility() checks CLI-specific paths
    - Uses Phase 1's getConfigPaths() for path resolution
    - Returns structured verification results
    - All functions documented with JSDoc
  </done>
</task>

<task type="auto">
  <name>Task 3: Create automated test suite</name>
  <files>bin/test-command-system.js</files>
  <action>
    Create `bin/test-command-system.js` as executable test suite for command system.
    
    Test structure:
    1. Test: Command Registry
       - Create registry, add command, retrieve command
       - Verify list() returns correct count
       - Verify has() returns true/false correctly
    
    2. Test: Argument Parser
       - Parse positionals: ['3'] → positionals: ['3']
       - Parse flags: ['--verbose'] → values: { verbose: true }
       - Parse combined: ['3', '--verbose', '--force'] → both
       - Parse with errors: invalid syntax → error field populated
    
    3. Test: Command Loader
       - Load from commands/gsd/
       - Verify 24 commands loaded
       - Check sample commands exist: help, execute-phase, new-project
    
    4. Test: Error Handler
       - Create CommandError with suggestions
       - Format error message (should include ❌ and suggestions)
       - Test degradeGracefully (should include ⚠️)
    
    5. Test: Help Generator
       - Generate full help (should list all commands grouped)
       - Generate specific help (should show command details)
    
    6. Test: Command Verifier
       - Run verifyCommands() (should pass with 24 commands)
    
    Format: Simple console.log() based assertions
    - ✅ Test name - on pass
    - ❌ Test name: error details - on fail
    - Exit with code 0 if all pass, 1 if any fail
    
    Add shebang: #!/usr/bin/env node
  </action>
  <verify>
    ```bash
    chmod +x bin/test-command-system.js
    node bin/test-command-system.js
    ```
    Should output: Series of ✅ marks and "All tests passed" message
  </verify>
  <done>
    - test-command-system.js exists and is executable
    - Tests registry operations (register, get, list, has)
    - Tests argument parsing with various inputs
    - Tests command loading from filesystem
    - Tests error formatting
    - Tests help generation
    - Tests verification functions
    - All tests pass with ✅ markers
    - Exits with appropriate code (0 = pass, 1 = fail)
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete command system infrastructure for unified CLI interface:
- Command registry with Map-based storage (bin/lib/command-system/registry.js)
- Argument parser using util.parseArgs() (bin/lib/command-system/parser.js)
- Dynamic command loader from filesystem (bin/lib/command-system/loader.js)
- Command executor with error handling (bin/lib/command-system/executor.js)
- Error handler with graceful degradation (bin/lib/command-system/error-handler.js)
- Auto-generated help system (bin/lib/command-system/help-generator.js)
- CLI entry point (bin/gsd-cli.js)
- Command recording system (bin/lib/command-system/recorder.js)
- Installation verifier (bin/lib/command-system/verifier.js)
- Automated test suite (bin/test-command-system.js)
  </what-built>
  <how-to-verify>
**Part 1: Unit tests (current directory)**

1. **Run automated test suite:**
   ```bash
   node bin/test-command-system.js
   ```
   ✓ All tests pass with ✅ markers
   ✓ Exit code is 0

2. **Test command verification:**
   ```bash
   node -e "
   import('./bin/lib/command-system/loader.js').then(m => m.loadCommands('commands/gsd')).then(() => {
     import('./bin/lib/command-system/verifier.js').then(v => v.verifyCommands());
   }).then(r => console.log('Commands verified:', r.commandCount, 'found'));
   "
   ```
   ✓ Should find 24 commands

**Part 2: Installation test (in /tmp)**

3. **Test Codex local installation:**
   ```bash
   mkdir -p /tmp/gsd-command-test
   cd /tmp/gsd-command-test
   node /Users/rodolfo/croonix-github/get-shit-done/bin/install.js --codex
   ```
   ✓ Installation completes without errors
   ✓ Files appear in `.codex/skills/get-shit-done/`

4. **Verify commands are accessible:**
   ```bash
   # Still in /tmp/gsd-command-test
   ls -la .codex/skills/get-shit-done/commands/gsd/ | wc -l
   ```
   ✓ Should show 24+ files (24 commands + directory entries)

5. **Test command invocation (if Codex CLI available):**
   ```bash
   # If you have Codex CLI installed:
   codex /gsd:help
   ```
   ✓ Shows command listing
   ✓ No errors about missing commands

6. **Test recording functionality:**
   ```bash
   cd /tmp/gsd-command-test
   node -e "
   import('/Users/rodolfo/croonix-github/get-shit-done/bin/lib/command-system/recorder.js').then(r => {
     return r.recordExecution('gsd:help', [], {success: true, output: 'test'}, 'codex-cli', 125);
   }).then(() => console.log('Recording test passed'));
   "
   ls -la .planning/command-recordings/
   ```
   ✓ Recording file created
   ✓ JSON format is valid

7. **Clean up:**
   ```bash
   rm -rf /tmp/gsd-command-test
   cd /Users/rodolfo/croonix-github/get-shit-done
   ```
  </how-to-verify>
  <what-success-looks-like>
- Unit tests pass in current directory (no side effects)
- Installation in /tmp completes successfully
- All 24 commands are accessible in installed location
- Command recording creates valid JSON files
- Verification confirms command system is complete and functional
- No errors or warnings during any test
  </what-success-looks-like>
  <if-issues>
**Common issues:**
- "Module not found" → Check that Phase 2 adapter files exist
- "24 commands expected, X found" → Run `ls commands/gsd/*.md | wc -l` to verify source
- "ENOENT .codex/skills" → Installation failed, check error message
- "Cannot find module recorder.js" → Ensure all files from tasks 1-3 were created

Document any issues found and their resolution in SUMMARY.md.
  </if-issues>
</task>

</tasks>

<verification>
**Automated verification (runs in current directory):**
```bash
# Run unit test suite
node bin/test-command-system.js
```

**Human verification (checkpoint above):**
- Part 1: Unit tests in current directory
- Part 2: Full installation test in /tmp/gsd-command-test
- All verifications must pass before plan is considered complete

Expected: All tests pass, installation works, 24 commands accessible.
</verification>

<success_criteria>
1. Command executions can be recorded to `.planning/command-recordings/` with JSON format including timestamp, CLI, command, args, result, and duration
2. User can load and compare recordings across different CLIs to identify behavioral differences
3. Verification confirms all 24 commands are accessible and properly formatted
4. Automated test suite validates registry, parser, loader, error handler, help generator, and verifier
5. Post-install verification (INSTALL-08) checks command accessibility in CLI-specific directories
6. All unit tests pass with clear ✅ / ❌ indicators
7. Installation test in /tmp completes successfully with all commands accessible
8. Recording and verification satisfy requirements CMD-05, CMD-06, and INSTALL-08
9. No side effects in current working directory (tests in /tmp only)
</success_criteria>

<output>
After completion, create `.planning/phases/03-command-system/03-03-SUMMARY.md` following the template structure with:
- Frontmatter (phase, plan, subsystem, dependency graph, tech tracking)
- Summary of recording and verification capabilities
- Key decisions made during implementation
- Performance metrics (duration, completion timestamp)
</output>
